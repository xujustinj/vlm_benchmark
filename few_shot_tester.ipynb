{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import importlib\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from SimilarityVLM import SimilarityVLM\n",
    "from dataset.dataset import FewShotTaskDataset, SequentialVideoDataset, SequentialCategoryNameDataset\n",
    "from FewShotClassifier import FewShotClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose VLM to Test\n",
    "\n",
    "Note, this notebook must be run using the corresponding conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLM = importlib.import_module(\"VT-TWINS.wrapper\").VTTWINS_SimilarityVLM\n",
    "vlm = VLM(reset_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Dataset to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SPLIT_PATH = \"/home/datasets/kinetics_100_split/test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling the Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da2b225ef1f44838df56dbbe19248cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_dataset = SequentialVideoDataset(DATASET_SPLIT_PATH)\n",
    "for vid_path in tqdm(video_dataset):\n",
    "    vlm.get_video_embeds(vid_path)\n",
    "    \n",
    "vlm.save_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_accuracy(classifier: FewShotClassifier, dataset_split_path: str, n_way: int, n_support: int, n_query: int = 1, n_episodes: int = 1000) -> float:\n",
    "    \n",
    "    # Load dataset to generate tasks with the desired params\n",
    "    dataset = FewShotTaskDataset(dataset_split_path, n_episodes, n_way, n_support, n_query)\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for vid_paths, category_names in tqdm(dataset):\n",
    "        \n",
    "        query_vid_paths = vid_paths[:, n_support:]\n",
    "        if n_support > 0:\n",
    "            support_vid_paths = vid_paths[:, :n_support]\n",
    "        else:\n",
    "            support_vid_paths = None\n",
    "            \n",
    "        query_predictions = classifier.predict(category_names, support_vid_paths, query_vid_paths)\n",
    "        \n",
    "        correct_predictions += np.sum(query_predictions == np.arange(n_way)[:, None])\n",
    "        total_queries += n_way * n_query\n",
    "        \n",
    "    return correct_predictions / total_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = FewShotClassifier(vlm, metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ecf8aa0e9342f8a1cc3c4b8dd24d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/vlm_benchmark/VT-TWINS/VT-TWINS/loader/msrvtt_loader.py:88: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352660876/work/torch/csrc/utils/tensor_numpy.cpp:172.)\n",
      "  video = th.from_numpy(video)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rob/vlm_benchmark/few_shot_tester.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/few_shot_tester.ipynb#ch0000009vscode-remote?line=0'>1</a>\u001b[0m few_shot_accuracy(classifier, DATASET_SPLIT_PATH, \u001b[39m5\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m20\u001b[39;49m)\n",
      "\u001b[1;32m/home/rob/vlm_benchmark/few_shot_tester.ipynb Cell 10\u001b[0m in \u001b[0;36mfew_shot_accuracy\u001b[0;34m(classifier, dataset_split_path, n_way, n_support, n_query, n_episodes)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/few_shot_tester.ipynb#ch0000009vscode-remote?line=12'>13</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/few_shot_tester.ipynb#ch0000009vscode-remote?line=13'>14</a>\u001b[0m     support_vid_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/few_shot_tester.ipynb#ch0000009vscode-remote?line=15'>16</a>\u001b[0m query_predictions \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(category_names, support_vid_paths, query_vid_paths)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/few_shot_tester.ipynb#ch0000009vscode-remote?line=17'>18</a>\u001b[0m correct_predictions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(query_predictions \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39marange(n_way)[:, \u001b[39mNone\u001b[39;00m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/few_shot_tester.ipynb#ch0000009vscode-remote?line=18'>19</a>\u001b[0m total_queries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m n_way \u001b[39m*\u001b[39m n_query\n",
      "File \u001b[0;32m~/vlm_benchmark/FewShotClassifier.py:57\u001b[0m, in \u001b[0;36mFewShotClassifier.predict\u001b[0;34m(self, category_names, support_video_paths, query_video_paths)\u001b[0m\n\u001b[1;32m     54\u001b[0m support_embeds\u001b[39m.\u001b[39mappend(text_embeds[:, \u001b[39mNone\u001b[39;00m, :])\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m n_support \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     flat_support_embeds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvlm\u001b[39m.\u001b[39mget_video_embeds(vid) \u001b[39mfor\u001b[39;00m vid \u001b[39min\u001b[39;00m support_video_paths\u001b[39m.\u001b[39mflatten()])\n\u001b[1;32m     58\u001b[0m     support_vid_embeds \u001b[39m=\u001b[39m flat_support_embeds\u001b[39m.\u001b[39mreshape(n_way, n_support, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m     support_embeds\u001b[39m.\u001b[39mappend(support_vid_embeds)\n",
      "File \u001b[0;32m~/vlm_benchmark/FewShotClassifier.py:57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m support_embeds\u001b[39m.\u001b[39mappend(text_embeds[:, \u001b[39mNone\u001b[39;00m, :])\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m n_support \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     flat_support_embeds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvlm\u001b[39m.\u001b[39;49mget_video_embeds(vid) \u001b[39mfor\u001b[39;00m vid \u001b[39min\u001b[39;00m support_video_paths\u001b[39m.\u001b[39mflatten()])\n\u001b[1;32m     58\u001b[0m     support_vid_embeds \u001b[39m=\u001b[39m flat_support_embeds\u001b[39m.\u001b[39mreshape(n_way, n_support, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m     support_embeds\u001b[39m.\u001b[39mappend(support_vid_embeds)\n",
      "File \u001b[0;32m~/vlm_benchmark/SimilarityVLM.py:91\u001b[0m, in \u001b[0;36mSimilarityVLM.get_video_embeds\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     89\u001b[0m video \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen_video(path)\n\u001b[1;32m     90\u001b[0m video \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(video)\n\u001b[0;32m---> 91\u001b[0m video_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvideo_encoder(video)\n\u001b[1;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache(path, video_embed)\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m video_embed\n",
      "File \u001b[0;32m~/vlm_benchmark/VT-TWINS/wrapper.py:72\u001b[0m, in \u001b[0;36mVTTWINS_SimilarityVLM.video_encoder\u001b[0;34m(self, video)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     71\u001b[0m     video \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward_video(video)\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/vlm_benchmark/VT-TWINS/VT-TWINS/s3dg.py:293\u001b[0m, in \u001b[0;36mS3D.forward_video\u001b[0;34m(self, inputs, mixed5c)\u001b[0m\n\u001b[1;32m    290\u001b[0m net \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixed_3b(net)\n\u001b[1;32m    291\u001b[0m \u001b[39m#out['Mixed_3b'] = net\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m# end_point = 'Mixed_3c'\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m net \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmixed_3c(net)\n\u001b[1;32m    294\u001b[0m \u001b[39m#out['Mixed_3c'] = net\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39m# end_point = 'MaxPool_4a_3x3'\u001b[39;00m\n\u001b[1;32m    296\u001b[0m net \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool_4a(net)\n",
      "File \u001b[0;32m~/miniconda3/envs/VLM_VTTWINS/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/vlm_benchmark/VT-TWINS/VT-TWINS/s3dg.py:35\u001b[0m, in \u001b[0;36mInceptionBlock.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     33\u001b[0m b0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_b0(\u001b[39minput\u001b[39m)\n\u001b[1;32m     34\u001b[0m b1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_b1_a(\u001b[39minput\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m b1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_b1_b(b1)\n\u001b[1;32m     36\u001b[0m b2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_b2_a(\u001b[39minput\u001b[39m)\n\u001b[1;32m     37\u001b[0m b2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_b2_b(b2)\n",
      "File \u001b[0;32m~/miniconda3/envs/VLM_VTTWINS/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/vlm_benchmark/VT-TWINS/VT-TWINS/s3dg.py:110\u001b[0m, in \u001b[0;36mSTConv3D.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(\u001b[39minput\u001b[39m)))\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseparable:\n\u001b[0;32m--> 110\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(out)))\n\u001b[1;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/VLM_VTTWINS/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/VLM_VTTWINS/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    177\u001b[0m     bn_training,\n\u001b[1;32m    178\u001b[0m     exponential_average_factor,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/VLM_VTTWINS/lib/python3.9/site-packages/torch/nn/functional.py:2438\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2436\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2438\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2439\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2440\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "few_shot_accuracy(classifier, DATASET_SPLIT_PATH, 5, 5, 2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('VLM_VTTWINS': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "531e74653c30fadb69f06333bfe9aca829cdb42de2b584f8d92e169bb9dc6b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
