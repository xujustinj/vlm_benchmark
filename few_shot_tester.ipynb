{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import importlib\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import dataset types\n",
    "from dataset.dataset import FewShotTaskDataset, SequentialVideoDataset, SequentialCategoryNameDataset\n",
    "\n",
    "# Import base classes\n",
    "from SimilarityVLM import SimilarityVLM\n",
    "from classifier.FewShotClassifier import FewShotClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Dataset to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SPLIT_PATH = \"/home/datasets/kinetics_100_split/test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Few-Shot Task Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAY = 5                       # Number of categories to choose between in each task\n",
    "N_SUPPORT = 10                  # Number of example videos per category per task\n",
    "N_QUERY = 1                     # Number of test videos per category per task\n",
    "N_EPISODES = 1000               # Number of few-shot tasks sampled in one iteration of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VLM and Few-Shot Classifier\n",
    "\n",
    "Note: This notebook must be run using the corresponding conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from VTTWINS.wrapper import VTTWINS_SimilarityVLM\n",
    "    vlm = VTTWINS_SimilarityVLM(reset_cache=False)\n",
    "    \n",
    "    from classifier.FewShotClassifier import FewShotClassifier\n",
    "    classifier = FewShotClassifier(vlm, metric=None)\n",
    "    \n",
    "if False:\n",
    "    from CLIP.CLIPVLM import ClipVLM\n",
    "    vlm = ClipVLM(reset_cache=False)\n",
    "    \n",
    "    from classifier.FewShotClassifier import FewShotClassifier\n",
    "    classifier = FewShotClassifier(vlm, metric=None)\n",
    "    \n",
    "if True:\n",
    "    from CLIP.CLIPVLM import ClipVLM\n",
    "    vlm = ClipVLM(reset_cache=False)\n",
    "    \n",
    "    from classifier.WeightedTextFewShotClassifier import WeightedTextFewShotClassifier\n",
    "    classifier = WeightedTextFewShotClassifier(vlm, metric=None, text_weight=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cb22b8fb0b4e65a05abc59d78c7da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_dataset = SequentialVideoDataset(DATASET_SPLIT_PATH)\n",
    "\n",
    "try:\n",
    "    for vid_path in tqdm(video_dataset):\n",
    "        if vid_path not in vlm.embed_cache:\n",
    "            vlm.get_video_embeds(vid_path)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    vlm.save_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup DataFrame for Saving Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RESULTS_PATH = \"test_results.csv\"\n",
    "TEST_RESULTS_COLUMNS = [\"vlm_class\", \"vlm_params\", \"classifier_class\", \"classifier_params\", \"dataset_split\", \"n_way\", \"n_support\", \"n_query\", \"n_episodes\", \"accuracy\"]\n",
    "\n",
    "if os.path.exists(TEST_RESULTS_PATH):\n",
    "    test_results = pd.read_csv(TEST_RESULTS_PATH)\n",
    "else:\n",
    "    test_results = pd.DataFrame(columns=TEST_RESULTS_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vlm_class</th>\n",
       "      <th>vlm_params</th>\n",
       "      <th>classifier_class</th>\n",
       "      <th>classifier_params</th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>n_way</th>\n",
       "      <th>n_support</th>\n",
       "      <th>n_query</th>\n",
       "      <th>n_episodes</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClipVLM</td>\n",
       "      <td>{\"path\": \"openai/clip-vit-base-patch32\", \"num_...</td>\n",
       "      <td>FewShotClassifier</td>\n",
       "      <td>{\"metric\": \"COSINE\"}</td>\n",
       "      <td>/home/datasets/kinetics_100_split/test.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VTTWINS_SimilarityVLM</td>\n",
       "      <td>{}</td>\n",
       "      <td>FewShotClassifier</td>\n",
       "      <td>{\"metric\": \"DOT\"}</td>\n",
       "      <td>/home/datasets/kinetics_100_split/test.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               vlm_class                                         vlm_params  \\\n",
       "0                ClipVLM  {\"path\": \"openai/clip-vit-base-patch32\", \"num_...   \n",
       "1  VTTWINS_SimilarityVLM                                                 {}   \n",
       "\n",
       "    classifier_class     classifier_params  \\\n",
       "0  FewShotClassifier  {\"metric\": \"COSINE\"}   \n",
       "1  FewShotClassifier     {\"metric\": \"DOT\"}   \n",
       "\n",
       "                                dataset_split  n_way  n_support  n_query  \\\n",
       "0  /home/datasets/kinetics_100_split/test.txt      5         10        1   \n",
       "1  /home/datasets/kinetics_100_split/test.txt      5         10        1   \n",
       "\n",
       "   n_episodes  accuracy  \n",
       "0        1000     0.830  \n",
       "1        1000     0.759  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Runs a few shot test using the given classifier, dataset, and task parameters.\n",
    "Returns the average accuracy over all sampled query videos in all sampled tasks.\n",
    "'''\n",
    "def few_shot_test(classifier: FewShotClassifier, dataset_split_path: str,\n",
    "                  n_way: int, n_support: int, n_query: int = 1, n_episodes: int = 1000) -> float:\n",
    "    \n",
    "    # Load dataset to generate tasks with the desired params\n",
    "    dataset = FewShotTaskDataset(dataset_split_path, n_episodes, n_way, n_support, n_query)\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_queries = 0\n",
    "    for vid_paths, category_names in tqdm(dataset):\n",
    "        \n",
    "        query_vid_paths = vid_paths[:, n_support:]\n",
    "        if n_support > 0:\n",
    "            support_vid_paths = vid_paths[:, :n_support]\n",
    "        else:\n",
    "            support_vid_paths = None\n",
    "            \n",
    "        query_predictions = classifier.predict(category_names, support_vid_paths, query_vid_paths)\n",
    "        \n",
    "        correct_predictions += np.sum(query_predictions == np.arange(n_way)[:, None])\n",
    "        total_queries += n_way * n_query\n",
    "    \n",
    "    return correct_predictions / total_queries\n",
    "\n",
    "'''\n",
    "Runs the given few-shot test if it has not already been performed,\n",
    "saving the result into a dataframe\n",
    "'''\n",
    "def collect_few_shot_test_results(test_results_df: pd.DataFrame,\n",
    "                                  classifier: FewShotClassifier, dataset_split_path: str,\n",
    "                                  n_way: int, n_support: int, n_query: int = 1, n_episodes: int = 1000,\n",
    "                                  ) -> None:\n",
    "    test_params = {\n",
    "        \"vlm_class\": classifier.vlm.__class__.__name__,\n",
    "        \"vlm_params\": json.dumps(classifier.vlm.params()),\n",
    "        \"classifier_class\": classifier.__class__.__name__,\n",
    "        \"classifier_params\": json.dumps(classifier.params()),\n",
    "        \"dataset_split\": dataset_split_path,\n",
    "        \"n_way\": n_way,\n",
    "        \"n_support\": n_support,\n",
    "        \"n_query\": n_query,\n",
    "        \"n_episodes\": n_episodes\n",
    "    }\n",
    "    \n",
    "    # Abort if test has already been recorded\n",
    "    filtered_tests = test_results_df\n",
    "    for key, val in test_params.items():\n",
    "        filtered_tests = filtered_tests[filtered_tests[key] == val]\n",
    "    if not filtered_tests.empty:\n",
    "        return\n",
    "    \n",
    "    # Run Test\n",
    "    accuracy = few_shot_test(classifier=classifier, dataset_split_path=dataset_split_path,\n",
    "                             n_way=n_way, n_support=n_support, n_query=n_query, n_episodes=n_episodes)\n",
    "    \n",
    "    # Save results\n",
    "    df_row = dict(test_params, accuracy=accuracy)\n",
    "    test_results_df.loc[len(test_results_df)] = df_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe1d820daf940f8a59727b83b54627a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collect_few_shot_test_results(test_results,\n",
    "                              classifier, DATASET_SPLIT_PATH,\n",
    "                              n_way=N_WAY, n_support=N_SUPPORT, n_query=N_QUERY, n_episodes=N_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Updated Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.to_csv(TEST_RESULTS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vlm_class</th>\n",
       "      <th>vlm_params</th>\n",
       "      <th>classifier_class</th>\n",
       "      <th>classifier_params</th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>n_way</th>\n",
       "      <th>n_support</th>\n",
       "      <th>n_query</th>\n",
       "      <th>n_episodes</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClipVLM</td>\n",
       "      <td>{\"path\": \"openai/clip-vit-base-patch32\", \"num_...</td>\n",
       "      <td>FewShotClassifier</td>\n",
       "      <td>{\"metric\": \"COSINE\"}</td>\n",
       "      <td>/home/datasets/kinetics_100_split/test.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VTTWINS_SimilarityVLM</td>\n",
       "      <td>{}</td>\n",
       "      <td>FewShotClassifier</td>\n",
       "      <td>{\"metric\": \"DOT\"}</td>\n",
       "      <td>/home/datasets/kinetics_100_split/test.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.7590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClipVLM</td>\n",
       "      <td>{\"path\": \"openai/clip-vit-base-patch32\", \"num_...</td>\n",
       "      <td>WeightedTextFewShotClassifier</td>\n",
       "      <td>{\"metric\": \"COSINE\", \"text_weight\": 4}</td>\n",
       "      <td>/home/datasets/kinetics_100_split/test.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.8558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               vlm_class                                         vlm_params  \\\n",
       "0                ClipVLM  {\"path\": \"openai/clip-vit-base-patch32\", \"num_...   \n",
       "1  VTTWINS_SimilarityVLM                                                 {}   \n",
       "2                ClipVLM  {\"path\": \"openai/clip-vit-base-patch32\", \"num_...   \n",
       "\n",
       "                classifier_class                       classifier_params  \\\n",
       "0              FewShotClassifier                    {\"metric\": \"COSINE\"}   \n",
       "1              FewShotClassifier                       {\"metric\": \"DOT\"}   \n",
       "2  WeightedTextFewShotClassifier  {\"metric\": \"COSINE\", \"text_weight\": 4}   \n",
       "\n",
       "                                dataset_split  n_way  n_support  n_query  \\\n",
       "0  /home/datasets/kinetics_100_split/test.txt      5         10        1   \n",
       "1  /home/datasets/kinetics_100_split/test.txt      5         10        1   \n",
       "2  /home/datasets/kinetics_100_split/test.txt      5         10        1   \n",
       "\n",
       "   n_episodes  accuracy  \n",
       "0        1000    0.8300  \n",
       "1        1000    0.7590  \n",
       "2        1000    0.8558  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('VLM_CLIP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cbb6134549cef05a22a5d2598d5d4563ecf4a78ae5860ae5dc90e185bb69e19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
