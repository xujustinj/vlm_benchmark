{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/vlm_benchmark/FewShotTestHandler.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import importlib\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from FewShotTestHandler import FewShotTestHandler, optimize_hyperparameters, find_hyperparameters, dataframe_format\n",
    "from dataset import DatasetHandler\n",
    "from similarity_metrics import Similarity\n",
    "from plotting_utils import plot\n",
    "\n",
    "ENV = os.environ[\"CONDA_DEFAULT_ENV\"]\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in /home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages (from scikit-optimize) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages (from scikit-optimize) (1.23.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages (from scikit-optimize) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages (from scikit-optimize) (1.1.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in /home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages (from scikit-optimize) (21.10.1)\n",
      "Requirement already satisfied: PyYAML in /home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize\n",
    "import skopt\n",
    "import skopt.plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH IS: video_clip/MMPT_updated/projects/retri/videoclip/how2.yaml\n",
      "CKPT SAVE DIR: video_clip/MMPT_updated/runs/retri/videoclip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MMBertForEncoder: ['bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.9.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'cls.predictions.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.bias']\n",
      "- This IS expected if you are initializing MMBertForEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MMBertForEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MMBertForEncoder were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['videomlp.LayerNorm.bias', 'videomlp.LayerNorm.weight', 'videomlp.linear1.bias', 'videomlp.linear2.weight', 'videomlp.linear1.weight', 'videomlp.linear2.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# VLM Params - vlm.____ keys are passed into VLM constructor\n",
    "if ENV == \"VLM_CLIP\":\n",
    "    from CLIP.CLIPVLM import ClipVLM as VLM\n",
    "    vlm = VLM(num_frames=10)\n",
    "elif ENV == \"VLM_MILES\":\n",
    "    from MILES.wrapper import MILES_SimilarityVLM as VLM\n",
    "    vlm = VLM()\n",
    "elif ENV == \"videoclip\":\n",
    "    from video_clip.video_clip import VideoClipVLM as VLM\n",
    "    vlm = VLM(num_seconds=4, sample_strat=\"spread\", use_cuda=True)\n",
    "elif ENV == \"VLM_UNIVL\":\n",
    "    from UNIVL.wrapper import UniVL_SimilarityVLM as VLM\n",
    "    vlm = VLM()\n",
    "elif ENV == \"VLM_VTTWINS\":\n",
    "    from VTTWINS.wrapper import VTTWINS_SimilarityVLM as VLM\n",
    "    vlm = VLM()\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from classifier import TipAdapterFewShotClassifier as Classifier\n",
    "    HYPERPARAM_SPACE = [\n",
    "        skopt.space.Real(0.1, 1000, name=\"alpha\", prior=\"log-uniform\"),\n",
    "        skopt.space.Real(0.1, 100, name=\"beta\", prior=\"log-uniform\"),\n",
    "        skopt.space.Real(1e-5, 1e-1, name=\"finetune_lr\", prior=\"log-uniform\"),\n",
    "        skopt.space.Integer(0, 30, name=\"finetune_epochs\"),\n",
    "        skopt.space.Real(1e-4, 1, name=\"weight_decay\", prior=\"log-uniform\")\n",
    "    ]\n",
    "if True:\n",
    "    from classifier.coop import CoopFewShotClassifier as Classifier\n",
    "    HYPERPARAM_SPACE = [\n",
    "        skopt.space.Real(1e-5, 1e-1, name=\"lr\", prior=\"log-uniform\"),\n",
    "        #skopt.space.Integer(8, 32, name=\"context_len\"),\n",
    "        skopt.space.Integer(0, 1, name=\"random_augment\"),\n",
    "        skopt.space.Integer(5, 30, name=\"epochs\"),\n",
    "        skopt.space.Integer(1, 8, name=\"batch_size\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc2e12e87f54ac296656de3d3c4b8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb8060b35904e3aaa5a79dc8122a500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET = \"smsm\"\n",
    "query_dataset = DatasetHandler(DATASET, \"val\")\n",
    "support_dataset = DatasetHandler(DATASET, \"train\")\n",
    "query_dataset.fill_cache(vlm)\n",
    "#support_dataset.fill_cache(vlm)\n",
    "\n",
    "N_WAY = 100\n",
    "N_SUPPORT_LIST = [4] #[0, 1, 2, 4, 8, 16, 32, 64]\n",
    "N_QUERY = None\n",
    "N_EPISODES = 4\n",
    "\n",
    "TESTHANDLER_RESULTS_FILE = f\"TEMP.hyperparam_search.{query_dataset.id()}.csv\"\n",
    "test_handler = FewShotTestHandler(TESTHANDLER_RESULTS_FILE)\n",
    "\n",
    "SKOPT_RESULTS_FILE = f\"TEMP.hyperparam_search.{vlm.__class__.__name__}.{query_dataset.id()}.{'_'.join([str(x) for x in N_SUPPORT_LIST])}_shot.pkl\"\n",
    "SKOPT_N_CALLS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SKOPT Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d860cb6fa17f4212b25b4474983cdf40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b22f561d274c11b0afbbbf1c3c3b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/transformers/modeling_utils.py:713: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:892: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  step = nonzero_hist[:-1].sum() // 255\n",
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:896: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  lut = (torch.cumsum(hist, 0) + (step // 2)) // step\n",
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: Acc = 0.025, Loss = 4.602\n",
      "Epoch     1: Acc = 0.020, Loss = 4.617\n",
      "Epoch     2: Acc = 0.022, Loss = 4.585\n",
      "Epoch     3: Acc = 0.018, Loss = 4.579\n",
      "Epoch     4: Acc = 0.018, Loss = 4.529\n",
      "Epoch     5: Acc = 0.030, Loss = 4.527\n",
      "Epoch     6: Acc = 0.025, Loss = 4.527\n",
      "Epoch     7: Acc = 0.030, Loss = 4.506\n",
      "Epoch     8: Acc = 0.037, Loss = 4.486\n",
      "Epoch     9: Acc = 0.020, Loss = 4.488\n",
      "Epoch    10: Acc = 0.027, Loss = 4.494\n",
      "Epoch    11: Acc = 0.043, Loss = 4.504\n",
      "Epoch    12: Acc = 0.015, Loss = 4.477\n",
      "Epoch    13: Acc = 0.020, Loss = 4.480\n",
      "Epoch    14: Acc = 0.040, Loss = 4.440\n",
      "Epoch    15: Acc = 0.035, Loss = 4.455\n",
      "Epoch    16: Acc = 0.055, Loss = 4.436\n",
      "Epoch    17: Acc = 0.030, Loss = 4.481\n",
      "Epoch     0: Acc = 0.027, Loss = 4.595\n",
      "Epoch     1: Acc = 0.030, Loss = 4.631\n",
      "Epoch     2: Acc = 0.030, Loss = 4.555\n",
      "Epoch     3: Acc = 0.047, Loss = 4.498\n",
      "Epoch     4: Acc = 0.027, Loss = 4.515\n",
      "Epoch     5: Acc = 0.043, Loss = 4.499\n",
      "Epoch     6: Acc = 0.032, Loss = 4.501\n",
      "Epoch     7: Acc = 0.052, Loss = 4.474\n",
      "Epoch     8: Acc = 0.035, Loss = 4.523\n",
      "Epoch     9: Acc = 0.032, Loss = 4.481\n",
      "Epoch    10: Acc = 0.040, Loss = 4.488\n",
      "Epoch    11: Acc = 0.035, Loss = 4.486\n",
      "Epoch    12: Acc = 0.040, Loss = 4.482\n",
      "Epoch    13: Acc = 0.040, Loss = 4.439\n",
      "Epoch    14: Acc = 0.047, Loss = 4.413\n",
      "Epoch    15: Acc = 0.062, Loss = 4.391\n",
      "Epoch    16: Acc = 0.047, Loss = 4.443\n",
      "Epoch    17: Acc = 0.060, Loss = 4.400\n",
      "Epoch     0: Acc = 0.027, Loss = 4.634\n",
      "Epoch     1: Acc = 0.027, Loss = 4.597\n",
      "Epoch     2: Acc = 0.040, Loss = 4.514\n",
      "Epoch     3: Acc = 0.027, Loss = 4.538\n",
      "Epoch     4: Acc = 0.040, Loss = 4.526\n",
      "Epoch     5: Acc = 0.035, Loss = 4.543\n",
      "Epoch     6: Acc = 0.040, Loss = 4.511\n",
      "Epoch     7: Acc = 0.022, Loss = 4.494\n",
      "Epoch     8: Acc = 0.040, Loss = 4.492\n",
      "Epoch     9: Acc = 0.050, Loss = 4.490\n",
      "Epoch    10: Acc = 0.043, Loss = 4.449\n",
      "Epoch    11: Acc = 0.045, Loss = 4.492\n",
      "Epoch    12: Acc = 0.032, Loss = 4.442\n",
      "Epoch    13: Acc = 0.052, Loss = 4.450\n",
      "Epoch    14: Acc = 0.060, Loss = 4.401\n",
      "Epoch    15: Acc = 0.043, Loss = 4.428\n",
      "Epoch    16: Acc = 0.067, Loss = 4.410\n",
      "Epoch    17: Acc = 0.045, Loss = 4.443\n",
      "Epoch     0: Acc = 0.018, Loss = 4.621\n",
      "Epoch     1: Acc = 0.015, Loss = 4.622\n",
      "Epoch     2: Acc = 0.012, Loss = 4.575\n",
      "Epoch     3: Acc = 0.035, Loss = 4.567\n",
      "Epoch     4: Acc = 0.035, Loss = 4.517\n",
      "Epoch     5: Acc = 0.037, Loss = 4.543\n",
      "Epoch     6: Acc = 0.030, Loss = 4.530\n",
      "Epoch     7: Acc = 0.035, Loss = 4.504\n",
      "Epoch     8: Acc = 0.045, Loss = 4.514\n",
      "Epoch     9: Acc = 0.030, Loss = 4.486\n",
      "Epoch    10: Acc = 0.032, Loss = 4.492\n",
      "Epoch    11: Acc = 0.035, Loss = 4.500\n",
      "Epoch    12: Acc = 0.043, Loss = 4.421\n",
      "Epoch    13: Acc = 0.032, Loss = 4.474\n",
      "Epoch    14: Acc = 0.025, Loss = 4.432\n",
      "Epoch    15: Acc = 0.037, Loss = 4.437\n",
      "Epoch    16: Acc = 0.040, Loss = 4.467\n",
      "Epoch    17: Acc = 0.027, Loss = 4.530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c720707a14e1454d80fa04f8d8a43002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/transformers/modeling_utils.py:713: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: Acc = 0.032, Loss = 4.523\n",
      "Epoch     1: Acc = 0.032, Loss = 4.521\n",
      "Epoch     2: Acc = 0.030, Loss = 4.519\n",
      "Epoch     3: Acc = 0.030, Loss = 4.516\n",
      "Epoch     4: Acc = 0.035, Loss = 4.514\n",
      "Epoch     5: Acc = 0.035, Loss = 4.512\n",
      "Epoch     6: Acc = 0.035, Loss = 4.510\n",
      "Epoch     7: Acc = 0.035, Loss = 4.509\n",
      "Epoch     8: Acc = 0.035, Loss = 4.507\n",
      "Epoch     9: Acc = 0.035, Loss = 4.507\n",
      "Epoch    10: Acc = 0.035, Loss = 4.506\n",
      "Epoch    11: Acc = 0.035, Loss = 4.506\n",
      "Epoch    12: Acc = 0.035, Loss = 4.506\n",
      "Epoch     0: Acc = 0.032, Loss = 4.551\n",
      "Epoch     1: Acc = 0.035, Loss = 4.548\n",
      "Epoch     2: Acc = 0.037, Loss = 4.544\n",
      "Epoch     3: Acc = 0.037, Loss = 4.539\n",
      "Epoch     4: Acc = 0.037, Loss = 4.535\n",
      "Epoch     5: Acc = 0.037, Loss = 4.532\n",
      "Epoch     6: Acc = 0.035, Loss = 4.528\n",
      "Epoch     7: Acc = 0.035, Loss = 4.526\n",
      "Epoch     8: Acc = 0.037, Loss = 4.524\n",
      "Epoch     9: Acc = 0.037, Loss = 4.523\n",
      "Epoch    10: Acc = 0.037, Loss = 4.522\n",
      "Epoch    11: Acc = 0.037, Loss = 4.521\n",
      "Epoch    12: Acc = 0.037, Loss = 4.521\n",
      "Epoch     0: Acc = 0.055, Loss = 4.521\n",
      "Epoch     1: Acc = 0.057, Loss = 4.519\n",
      "Epoch     2: Acc = 0.055, Loss = 4.516\n",
      "Epoch     3: Acc = 0.057, Loss = 4.512\n",
      "Epoch     4: Acc = 0.057, Loss = 4.509\n",
      "Epoch     5: Acc = 0.060, Loss = 4.506\n",
      "Epoch     6: Acc = 0.060, Loss = 4.504\n",
      "Epoch     7: Acc = 0.060, Loss = 4.502\n",
      "Epoch     8: Acc = 0.060, Loss = 4.500\n",
      "Epoch     9: Acc = 0.062, Loss = 4.499\n",
      "Epoch    10: Acc = 0.062, Loss = 4.499\n",
      "Epoch    11: Acc = 0.062, Loss = 4.498\n",
      "Epoch    12: Acc = 0.062, Loss = 4.498\n",
      "Epoch     0: Acc = 0.045, Loss = 4.496\n",
      "Epoch     1: Acc = 0.045, Loss = 4.495\n",
      "Epoch     2: Acc = 0.045, Loss = 4.493\n",
      "Epoch     3: Acc = 0.045, Loss = 4.491\n",
      "Epoch     4: Acc = 0.045, Loss = 4.490\n",
      "Epoch     5: Acc = 0.047, Loss = 4.488\n",
      "Epoch     6: Acc = 0.047, Loss = 4.487\n",
      "Epoch     7: Acc = 0.047, Loss = 4.486\n",
      "Epoch     8: Acc = 0.047, Loss = 4.485\n",
      "Epoch     9: Acc = 0.047, Loss = 4.485\n",
      "Epoch    10: Acc = 0.047, Loss = 4.485\n",
      "Epoch    11: Acc = 0.047, Loss = 4.484\n",
      "Epoch    12: Acc = 0.047, Loss = 4.484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2804599cb8ad4ca28216d5aaa8af0bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: Acc = 0.040, Loss = 4.493\n",
      "Epoch     1: Acc = 0.037, Loss = 4.448\n",
      "Epoch     2: Acc = 0.067, Loss = 4.354\n",
      "Epoch     3: Acc = 0.065, Loss = 4.297\n",
      "Epoch     4: Acc = 0.065, Loss = 4.253\n",
      "Epoch     5: Acc = 0.065, Loss = 4.209\n",
      "Epoch     6: Acc = 0.062, Loss = 4.165\n",
      "Epoch     7: Acc = 0.057, Loss = 4.121\n",
      "Epoch     8: Acc = 0.065, Loss = 4.075\n",
      "Epoch     9: Acc = 0.067, Loss = 4.031\n",
      "Epoch    10: Acc = 0.078, Loss = 3.991\n",
      "Epoch    11: Acc = 0.082, Loss = 3.957\n",
      "Epoch    12: Acc = 0.090, Loss = 3.928\n",
      "Epoch    13: Acc = 0.095, Loss = 3.905\n",
      "Epoch    14: Acc = 0.093, Loss = 3.890\n",
      "Epoch    15: Acc = 0.100, Loss = 3.882\n",
      "Epoch     0: Acc = 0.037, Loss = 4.517\n",
      "Epoch     1: Acc = 0.045, Loss = 4.483\n",
      "Epoch     2: Acc = 0.037, Loss = 4.411\n",
      "Epoch     3: Acc = 0.045, Loss = 4.366\n",
      "Epoch     4: Acc = 0.052, Loss = 4.327\n",
      "Epoch     5: Acc = 0.070, Loss = 4.286\n",
      "Epoch     6: Acc = 0.082, Loss = 4.242\n",
      "Epoch     7: Acc = 0.075, Loss = 4.198\n",
      "Epoch     8: Acc = 0.085, Loss = 4.149\n",
      "Epoch     9: Acc = 0.085, Loss = 4.105\n",
      "Epoch    10: Acc = 0.095, Loss = 4.059\n",
      "Epoch    11: Acc = 0.107, Loss = 4.021\n",
      "Epoch    12: Acc = 0.115, Loss = 3.988\n",
      "Epoch    13: Acc = 0.107, Loss = 3.965\n",
      "Epoch    14: Acc = 0.115, Loss = 3.947\n",
      "Epoch    15: Acc = 0.117, Loss = 3.938\n",
      "Epoch     0: Acc = 0.040, Loss = 4.563\n",
      "Epoch     1: Acc = 0.050, Loss = 4.480\n",
      "Epoch     2: Acc = 0.057, Loss = 4.392\n",
      "Epoch     3: Acc = 0.057, Loss = 4.344\n",
      "Epoch     4: Acc = 0.072, Loss = 4.301\n",
      "Epoch     5: Acc = 0.080, Loss = 4.258\n",
      "Epoch     6: Acc = 0.082, Loss = 4.214\n",
      "Epoch     7: Acc = 0.080, Loss = 4.174\n",
      "Epoch     8: Acc = 0.093, Loss = 4.135\n",
      "Epoch     9: Acc = 0.107, Loss = 4.107\n",
      "Epoch    10: Acc = 0.102, Loss = 4.078\n",
      "Epoch    11: Acc = 0.110, Loss = 4.053\n",
      "Epoch    12: Acc = 0.120, Loss = 4.033\n",
      "Epoch    13: Acc = 0.117, Loss = 4.017\n",
      "Epoch    14: Acc = 0.117, Loss = 4.007\n",
      "Epoch    15: Acc = 0.117, Loss = 4.002\n",
      "Epoch     0: Acc = 0.040, Loss = 4.511\n",
      "Epoch     1: Acc = 0.045, Loss = 4.457\n",
      "Epoch     2: Acc = 0.060, Loss = 4.375\n",
      "Epoch     3: Acc = 0.072, Loss = 4.324\n",
      "Epoch     4: Acc = 0.075, Loss = 4.283\n",
      "Epoch     5: Acc = 0.072, Loss = 4.244\n",
      "Epoch     6: Acc = 0.080, Loss = 4.212\n",
      "Epoch     7: Acc = 0.080, Loss = 4.183\n",
      "Epoch     8: Acc = 0.087, Loss = 4.152\n",
      "Epoch     9: Acc = 0.087, Loss = 4.123\n",
      "Epoch    10: Acc = 0.087, Loss = 4.097\n",
      "Epoch    11: Acc = 0.100, Loss = 4.074\n",
      "Epoch    12: Acc = 0.107, Loss = 4.055\n",
      "Epoch    13: Acc = 0.110, Loss = 4.041\n",
      "Epoch    14: Acc = 0.112, Loss = 4.031\n",
      "Epoch    15: Acc = 0.110, Loss = 4.026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc42769357754c4e9178183421494d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: Acc = 0.030, Loss = 4.519\n",
      "Epoch     1: Acc = 0.052, Loss = 4.384\n",
      "Epoch     2: Acc = 0.060, Loss = 4.261\n",
      "Epoch     3: Acc = 0.070, Loss = 4.177\n",
      "Epoch     4: Acc = 0.075, Loss = 4.106\n",
      "Epoch     5: Acc = 0.090, Loss = 4.043\n",
      "Epoch     6: Acc = 0.110, Loss = 3.949\n",
      "Epoch     7: Acc = 0.122, Loss = 3.875\n",
      "Epoch     8: Acc = 0.147, Loss = 3.803\n",
      "Epoch     9: Acc = 0.165, Loss = 3.705\n",
      "Epoch    10: Acc = 0.162, Loss = 3.599\n",
      "Epoch    11: Acc = 0.170, Loss = 3.512\n",
      "Epoch    12: Acc = 0.205, Loss = 3.422\n",
      "Epoch    13: Acc = 0.240, Loss = 3.336\n",
      "Epoch    14: Acc = 0.255, Loss = 3.266\n",
      "Epoch    15: Acc = 0.265, Loss = 3.211\n",
      "Epoch    16: Acc = 0.273, Loss = 3.176\n",
      "Epoch     0: Acc = 0.040, Loss = 4.534\n",
      "Epoch     1: Acc = 0.047, Loss = 4.421\n",
      "Epoch     2: Acc = 0.062, Loss = 4.300\n",
      "Epoch     3: Acc = 0.065, Loss = 4.229\n",
      "Epoch     4: Acc = 0.067, Loss = 4.163\n",
      "Epoch     5: Acc = 0.090, Loss = 4.099\n",
      "Epoch     6: Acc = 0.100, Loss = 4.023\n",
      "Epoch     7: Acc = 0.115, Loss = 3.957\n",
      "Epoch     8: Acc = 0.120, Loss = 3.875\n",
      "Epoch     9: Acc = 0.142, Loss = 3.788\n",
      "Epoch    10: Acc = 0.135, Loss = 3.707\n",
      "Epoch    11: Acc = 0.175, Loss = 3.623\n",
      "Epoch    12: Acc = 0.197, Loss = 3.527\n",
      "Epoch    13: Acc = 0.195, Loss = 3.441\n",
      "Epoch    14: Acc = 0.215, Loss = 3.363\n",
      "Epoch    15: Acc = 0.215, Loss = 3.305\n",
      "Epoch    16: Acc = 0.237, Loss = 3.266\n",
      "Epoch     0: Acc = 0.047, Loss = 4.517\n",
      "Epoch     1: Acc = 0.062, Loss = 4.384\n",
      "Epoch     2: Acc = 0.070, Loss = 4.254\n",
      "Epoch     3: Acc = 0.075, Loss = 4.171\n",
      "Epoch     4: Acc = 0.082, Loss = 4.128\n",
      "Epoch     5: Acc = 0.090, Loss = 4.050\n",
      "Epoch     6: Acc = 0.110, Loss = 3.979\n",
      "Epoch     7: Acc = 0.105, Loss = 3.891\n",
      "Epoch     8: Acc = 0.132, Loss = 3.800\n",
      "Epoch     9: Acc = 0.147, Loss = 3.732\n",
      "Epoch    10: Acc = 0.153, Loss = 3.672\n",
      "Epoch    11: Acc = 0.165, Loss = 3.591\n",
      "Epoch    12: Acc = 0.160, Loss = 3.505\n",
      "Epoch    13: Acc = 0.177, Loss = 3.428\n",
      "Epoch    14: Acc = 0.197, Loss = 3.362\n",
      "Epoch    15: Acc = 0.212, Loss = 3.312\n",
      "Epoch    16: Acc = 0.212, Loss = 3.283\n",
      "Epoch     0: Acc = 0.043, Loss = 4.535\n",
      "Epoch     1: Acc = 0.055, Loss = 4.448\n",
      "Epoch     2: Acc = 0.078, Loss = 4.327\n",
      "Epoch     3: Acc = 0.085, Loss = 4.232\n",
      "Epoch     4: Acc = 0.097, Loss = 4.138\n",
      "Epoch     5: Acc = 0.102, Loss = 4.059\n",
      "Epoch     6: Acc = 0.120, Loss = 3.965\n",
      "Epoch     7: Acc = 0.112, Loss = 3.901\n",
      "Epoch     8: Acc = 0.135, Loss = 3.803\n",
      "Epoch     9: Acc = 0.162, Loss = 3.723\n",
      "Epoch    10: Acc = 0.165, Loss = 3.640\n",
      "Epoch    11: Acc = 0.195, Loss = 3.555\n",
      "Epoch    12: Acc = 0.215, Loss = 3.459\n",
      "Epoch    13: Acc = 0.222, Loss = 3.370\n",
      "Epoch    14: Acc = 0.252, Loss = 3.288\n",
      "Epoch    15: Acc = 0.255, Loss = 3.225\n",
      "Epoch    16: Acc = 0.270, Loss = 3.185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d957c5a6e1f45cf821a6e5c3488e45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:892: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  step = nonzero_hist[:-1].sum() // 255\n",
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:896: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  lut = (torch.cumsum(hist, 0) + (step // 2)) // step\n",
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/transformers/modeling_utils.py:713: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: Acc = 0.020, Loss = 4.601\n",
      "Epoch     1: Acc = 0.010, Loss = 4.609\n",
      "Epoch     2: Acc = 0.022, Loss = 4.582\n",
      "Epoch     3: Acc = 0.035, Loss = 4.566\n",
      "Epoch     4: Acc = 0.025, Loss = 4.541\n",
      "Epoch     5: Acc = 0.025, Loss = 4.515\n",
      "Epoch     6: Acc = 0.025, Loss = 4.532\n",
      "Epoch     7: Acc = 0.027, Loss = 4.526\n",
      "Epoch     8: Acc = 0.040, Loss = 4.470\n",
      "Epoch     9: Acc = 0.025, Loss = 4.518\n",
      "Epoch    10: Acc = 0.018, Loss = 4.577\n",
      "Epoch    11: Acc = 0.030, Loss = 4.488\n",
      "Epoch    12: Acc = 0.025, Loss = 4.505\n",
      "Epoch    13: Acc = 0.037, Loss = 4.441\n",
      "Epoch    14: Acc = 0.030, Loss = 4.488\n",
      "Epoch     0: Acc = 0.020, Loss = 4.677\n",
      "Epoch     1: Acc = 0.018, Loss = 4.615\n",
      "Epoch     2: Acc = 0.025, Loss = 4.590\n",
      "Epoch     3: Acc = 0.030, Loss = 4.546\n",
      "Epoch     4: Acc = 0.022, Loss = 4.558\n",
      "Epoch     5: Acc = 0.022, Loss = 4.547\n",
      "Epoch     6: Acc = 0.020, Loss = 4.575\n",
      "Epoch     7: Acc = 0.043, Loss = 4.530\n",
      "Epoch     8: Acc = 0.032, Loss = 4.527\n",
      "Epoch     9: Acc = 0.030, Loss = 4.547\n",
      "Epoch    10: Acc = 0.040, Loss = 4.513\n",
      "Epoch    11: Acc = 0.037, Loss = 4.551\n",
      "Epoch    12: Acc = 0.040, Loss = 4.497\n",
      "Epoch    13: Acc = 0.045, Loss = 4.532\n",
      "Epoch    14: Acc = 0.035, Loss = 4.535\n",
      "Epoch     0: Acc = 0.020, Loss = 4.605\n",
      "Epoch     1: Acc = 0.037, Loss = 4.570\n",
      "Epoch     2: Acc = 0.032, Loss = 4.573\n",
      "Epoch     3: Acc = 0.040, Loss = 4.574\n",
      "Epoch     4: Acc = 0.030, Loss = 4.589\n",
      "Epoch     5: Acc = 0.030, Loss = 4.556\n",
      "Epoch     6: Acc = 0.035, Loss = 4.516\n",
      "Epoch     7: Acc = 0.037, Loss = 4.532\n",
      "Epoch     8: Acc = 0.035, Loss = 4.556\n",
      "Epoch     9: Acc = 0.035, Loss = 4.522\n",
      "Epoch    10: Acc = 0.025, Loss = 4.549\n",
      "Epoch    11: Acc = 0.032, Loss = 4.505\n",
      "Epoch    12: Acc = 0.040, Loss = 4.477\n",
      "Epoch    13: Acc = 0.030, Loss = 4.516\n",
      "Epoch    14: Acc = 0.037, Loss = 4.516\n",
      "Epoch     0: Acc = 0.010, Loss = 4.628\n",
      "Epoch     1: Acc = 0.010, Loss = 4.650\n",
      "Epoch     2: Acc = 0.022, Loss = 4.614\n",
      "Epoch     3: Acc = 0.020, Loss = 4.548\n",
      "Epoch     4: Acc = 0.030, Loss = 4.538\n",
      "Epoch     5: Acc = 0.018, Loss = 4.554\n",
      "Epoch     6: Acc = 0.035, Loss = 4.570\n",
      "Epoch     7: Acc = 0.040, Loss = 4.540\n",
      "Epoch     8: Acc = 0.022, Loss = 4.536\n",
      "Epoch     9: Acc = 0.030, Loss = 4.505\n",
      "Epoch    10: Acc = 0.025, Loss = 4.543\n",
      "Epoch    11: Acc = 0.022, Loss = 4.549\n",
      "Epoch    12: Acc = 0.022, Loss = 4.495\n",
      "Epoch    13: Acc = 0.037, Loss = 4.496\n",
      "Epoch    14: Acc = 0.020, Loss = 4.502\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec38609662984649bf1f151ad6ffca14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:892: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  step = nonzero_hist[:-1].sum() // 255\n",
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:896: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  lut = (torch.cumsum(hist, 0) + (step // 2)) // step\n",
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/transformers/modeling_utils.py:713: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: Acc = 0.025, Loss = 4.612\n",
      "Epoch     1: Acc = 0.010, Loss = 4.591\n",
      "Epoch     2: Acc = 0.032, Loss = 4.527\n",
      "Epoch     3: Acc = 0.035, Loss = 4.467\n",
      "Epoch     4: Acc = 0.020, Loss = 4.483\n",
      "Epoch     5: Acc = 0.045, Loss = 4.496\n",
      "Epoch     6: Acc = 0.062, Loss = 4.446\n",
      "Epoch     7: Acc = 0.040, Loss = 4.404\n",
      "Epoch     8: Acc = 0.047, Loss = 4.396\n",
      "Epoch     9: Acc = 0.045, Loss = 4.353\n",
      "Epoch    10: Acc = 0.050, Loss = 4.347\n",
      "Epoch     0: Acc = 0.010, Loss = 4.616\n",
      "Epoch     1: Acc = 0.043, Loss = 4.583\n",
      "Epoch     2: Acc = 0.015, Loss = 4.583\n",
      "Epoch     3: Acc = 0.032, Loss = 4.556\n",
      "Epoch     4: Acc = 0.030, Loss = 4.533\n",
      "Epoch     5: Acc = 0.032, Loss = 4.487\n",
      "Epoch     6: Acc = 0.047, Loss = 4.464\n",
      "Epoch     7: Acc = 0.035, Loss = 4.483\n",
      "Epoch     8: Acc = 0.040, Loss = 4.422\n",
      "Epoch     9: Acc = 0.045, Loss = 4.429\n",
      "Epoch    10: Acc = 0.015, Loss = 4.441\n",
      "Epoch     0: Acc = 0.022, Loss = 4.645\n",
      "Epoch     1: Acc = 0.020, Loss = 4.633\n",
      "Epoch     2: Acc = 0.012, Loss = 4.570\n",
      "Epoch     3: Acc = 0.045, Loss = 4.533\n",
      "Epoch     4: Acc = 0.045, Loss = 4.489\n",
      "Epoch     5: Acc = 0.020, Loss = 4.550\n",
      "Epoch     6: Acc = 0.035, Loss = 4.504\n",
      "Epoch     7: Acc = 0.032, Loss = 4.444\n",
      "Epoch     8: Acc = 0.037, Loss = 4.438\n",
      "Epoch     9: Acc = 0.032, Loss = 4.429\n",
      "Epoch    10: Acc = 0.047, Loss = 4.350\n",
      "Epoch     0: Acc = 0.018, Loss = 4.675\n",
      "Epoch     1: Acc = 0.015, Loss = 4.594\n",
      "Epoch     2: Acc = 0.032, Loss = 4.533\n",
      "Epoch     3: Acc = 0.045, Loss = 4.523\n",
      "Epoch     4: Acc = 0.035, Loss = 4.473\n",
      "Epoch     5: Acc = 0.030, Loss = 4.466\n",
      "Epoch     6: Acc = 0.050, Loss = 4.431\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rob/vlm_benchmark/hyperparam_search.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     y0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m skopt_pbar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39mSKOPT_N_CALLS) \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m skopt_results \u001b[39m=\u001b[39m skopt\u001b[39m.\u001b[39;49mgp_minimize(val_neg_accuracy, HYPERPARAM_SPACE, n_calls\u001b[39m=\u001b[39;49mSKOPT_N_CALLS, callback\u001b[39m=\u001b[39;49mcallback, x0\u001b[39m=\u001b[39;49mx0, y0\u001b[39m=\u001b[39;49my0)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/skopt/optimizer/gp.py:259\u001b[0m, in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m base_estimator \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     base_estimator \u001b[39m=\u001b[39m cook_estimator(\n\u001b[1;32m    256\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGP\u001b[39m\u001b[39m\"\u001b[39m, space\u001b[39m=\u001b[39mspace, random_state\u001b[39m=\u001b[39mrng\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax),\n\u001b[1;32m    257\u001b[0m         noise\u001b[39m=\u001b[39mnoise)\n\u001b[0;32m--> 259\u001b[0m \u001b[39mreturn\u001b[39;00m base_minimize(\n\u001b[1;32m    260\u001b[0m     func, space, base_estimator\u001b[39m=\u001b[39;49mbase_estimator,\n\u001b[1;32m    261\u001b[0m     acq_func\u001b[39m=\u001b[39;49macq_func,\n\u001b[1;32m    262\u001b[0m     xi\u001b[39m=\u001b[39;49mxi, kappa\u001b[39m=\u001b[39;49mkappa, acq_optimizer\u001b[39m=\u001b[39;49macq_optimizer, n_calls\u001b[39m=\u001b[39;49mn_calls,\n\u001b[1;32m    263\u001b[0m     n_points\u001b[39m=\u001b[39;49mn_points, n_random_starts\u001b[39m=\u001b[39;49mn_random_starts,\n\u001b[1;32m    264\u001b[0m     n_initial_points\u001b[39m=\u001b[39;49mn_initial_points,\n\u001b[1;32m    265\u001b[0m     initial_point_generator\u001b[39m=\u001b[39;49minitial_point_generator,\n\u001b[1;32m    266\u001b[0m     n_restarts_optimizer\u001b[39m=\u001b[39;49mn_restarts_optimizer,\n\u001b[1;32m    267\u001b[0m     x0\u001b[39m=\u001b[39;49mx0, y0\u001b[39m=\u001b[39;49my0, random_state\u001b[39m=\u001b[39;49mrng, verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    268\u001b[0m     callback\u001b[39m=\u001b[39;49mcallback, n_jobs\u001b[39m=\u001b[39;49mn_jobs, model_queue_size\u001b[39m=\u001b[39;49mmodel_queue_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/skopt/optimizer/base.py:299\u001b[0m, in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_calls):\n\u001b[1;32m    298\u001b[0m     next_x \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mask()\n\u001b[0;32m--> 299\u001b[0m     next_y \u001b[39m=\u001b[39m func(next_x)\n\u001b[1;32m    300\u001b[0m     result \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mtell(next_x, next_y)\n\u001b[1;32m    301\u001b[0m     result\u001b[39m.\u001b[39mspecs \u001b[39m=\u001b[39m specs\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/skopt/utils.py:789\u001b[0m, in \u001b[0;36muse_named_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    786\u001b[0m arg_dict \u001b[39m=\u001b[39m {dim\u001b[39m.\u001b[39mname: value \u001b[39mfor\u001b[39;00m dim, value \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(dimensions, x)}\n\u001b[1;32m    788\u001b[0m \u001b[39m# Call the wrapped objective function with the named arguments.\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m objective_value \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49marg_dict)\n\u001b[1;32m    791\u001b[0m \u001b[39mreturn\u001b[39;00m objective_value\n",
      "\u001b[1;32m/home/rob/vlm_benchmark/hyperparam_search.ipynb Cell 8\u001b[0m in \u001b[0;36mval_neg_accuracy\u001b[0;34m(**classifier_params)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m accuracies \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m n_support \u001b[39min\u001b[39;00m N_SUPPORT_LIST:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     test_handler\u001b[39m.\u001b[39;49mrun_few_shot_test(classifier, query_dataset, support_dataset, N_WAY, n_support, N_QUERY, N_EPISODES)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Find dataframe row with accuracy from this run\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     filter_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(test_handler\u001b[39m.\u001b[39mresults))\n",
      "File \u001b[0;32m~/vlm_benchmark/FewShotTestHandler.py:65\u001b[0m, in \u001b[0;36mFewShotTestHandler.run_few_shot_test\u001b[0;34m(self, classifier, query_dataset, support_dataset, n_way, n_support, n_query, n_episodes)\u001b[0m\n\u001b[1;32m     62\u001b[0m dataset_iter \u001b[39m=\u001b[39m tqdm(few_shot_dataset, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[39mfor\u001b[39;00m category_names, support_vid_paths, query_vid_paths, query_vid_labels \u001b[39min\u001b[39;00m dataset_iter:\n\u001b[0;32m---> 65\u001b[0m     query_predictions \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(category_names, support_vid_paths, query_vid_paths)\n\u001b[1;32m     67\u001b[0m     \u001b[39m# Compute accuracy for this sampled task\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     correct_predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(query_predictions \u001b[39m==\u001b[39m query_vid_labels)\n",
      "File \u001b[0;32m~/vlm_benchmark/classifier/coop.py:116\u001b[0m, in \u001b[0;36mCoopFewShotClassifier.predict\u001b[0;34m(self, category_names, support_video_paths, query_video_paths)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (vid_paths, vid_labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_augment:\n\u001b[0;32m--> 116\u001b[0m         vid_embeds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[1;32m    117\u001b[0m             torch\u001b[39m.\u001b[39mfrom_numpy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvlm\u001b[39m.\u001b[39mvideo_encoder(vid_path, random_augment\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m    118\u001b[0m             \u001b[39mfor\u001b[39;00m vid_path \u001b[39min\u001b[39;00m vid_paths\n\u001b[1;32m    119\u001b[0m         ], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    120\u001b[0m     \u001b[39melse\u001b[39;00m: \u001b[39m# Use version of video encoder which can cache results for fast lookup\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         vid_embeds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[1;32m    122\u001b[0m             torch\u001b[39m.\u001b[39mfrom_numpy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvlm\u001b[39m.\u001b[39mget_video_embeds(vid_path))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m    123\u001b[0m             \u001b[39mfor\u001b[39;00m vid_path \u001b[39min\u001b[39;00m vid_paths\n\u001b[1;32m    124\u001b[0m         ], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/vlm_benchmark/classifier/coop.py:117\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (vid_paths, vid_labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_augment:\n\u001b[1;32m    116\u001b[0m         vid_embeds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[0;32m--> 117\u001b[0m             torch\u001b[39m.\u001b[39mfrom_numpy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvlm\u001b[39m.\u001b[39;49mvideo_encoder(vid_path, random_augment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m    118\u001b[0m             \u001b[39mfor\u001b[39;00m vid_path \u001b[39min\u001b[39;00m vid_paths\n\u001b[1;32m    119\u001b[0m         ], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    120\u001b[0m     \u001b[39melse\u001b[39;00m: \u001b[39m# Use version of video encoder which can cache results for fast lookup\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         vid_embeds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[1;32m    122\u001b[0m             torch\u001b[39m.\u001b[39mfrom_numpy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvlm\u001b[39m.\u001b[39mget_video_embeds(vid_path))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m    123\u001b[0m             \u001b[39mfor\u001b[39;00m vid_path \u001b[39min\u001b[39;00m vid_paths\n\u001b[1;32m    124\u001b[0m         ], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/vlm_benchmark/video_clip/video_clip.py:276\u001b[0m, in \u001b[0;36mVideoClipVLM.video_encoder\u001b[0;34m(self, video_path, subvideo_start_frame, subvideo_end_frame, random_augment)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39mLoad, transform and encode a video file into a joint text/video embedding space\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m:param video:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m video \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen_video(video_path, subvideo_start_frame, subvideo_end_frame, random_augment)\n\u001b[0;32m--> 276\u001b[0m video \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(video, random_augment)\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcuda:\n\u001b[1;32m    279\u001b[0m     video \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m~/vlm_benchmark/video_clip/video_clip.py:259\u001b[0m, in \u001b[0;36mVideoClipVLM.transform\u001b[0;34m(self, video, random_augment)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mTransforms video using model-specific transforms\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m:param video:\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m random_augment:\n\u001b[0;32m--> 259\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_transforms(video)\n\u001b[1;32m    260\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms(video)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torchvision/transforms/transforms.py:60\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     59\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 60\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/pytorchvideo/transforms/rand_augment.py:101\u001b[0m, in \u001b[0;36mRandAugment.__call__\u001b[0;34m(self, video)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, video: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     95\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    Perform RandAugment to the input video tensor.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m        video (torch.Tensor): Input video tensor with shape (T, C, H, W).\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandaug_fn(video)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/pytorchvideo/transforms/transforms.py:410\u001b[0m, in \u001b[0;36mOpSampler.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    405\u001b[0m index_list \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmultinomial(\n\u001b[1;32m    406\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms_prob, depth, replacement\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement\n\u001b[1;32m    407\u001b[0m )\n\u001b[1;32m    409\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m index_list:\n\u001b[0;32m--> 410\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms_list[index](x)\n\u001b[1;32m    412\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/pytorchvideo/transforms/augmentations.py:482\u001b[0m, in \u001b[0;36mAugmentTransform.__call__\u001b[0;34m(self, video)\u001b[0m\n\u001b[1;32m    476\u001b[0m magnitude \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_magnitude()\n\u001b[1;32m    477\u001b[0m level_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    478\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevel_fn(magnitude, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevel_paras)\n\u001b[1;32m    479\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevel_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    480\u001b[0m     \u001b[39melse\u001b[39;00m ()\n\u001b[1;32m    481\u001b[0m )\n\u001b[0;32m--> 482\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_fn(video, \u001b[39m*\u001b[39;49mlevel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_hparas)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/pytorchvideo/transforms/augmentations.py:136\u001b[0m, in \u001b[0;36m_adjust_sharpness\u001b[0;34m(video, factor, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_adjust_sharpness\u001b[39m(video: torch\u001b[39m.\u001b[39mTensor, factor: \u001b[39mfloat\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    127\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    Adjust the sharpness of a video.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39m            increases the sharpness by a factor of 2.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m torchvision\u001b[39m.\u001b[39;49mtransforms\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49madjust_sharpness(video, factor)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torchvision/transforms/functional.py:1284\u001b[0m, in \u001b[0;36madjust_sharpness\u001b[0;34m(img, sharpness_factor)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m   1282\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39madjust_sharpness(img, sharpness_factor)\n\u001b[0;32m-> 1284\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49madjust_sharpness(img, sharpness_factor)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:863\u001b[0m, in \u001b[0;36madjust_sharpness\u001b[0;34m(img, sharpness_factor)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[39mif\u001b[39;00m img\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m img\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    861\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n\u001b[0;32m--> 863\u001b[0m \u001b[39mreturn\u001b[39;00m _blend(img, _blurred_degenerate_image(img), sharpness_factor)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:311\u001b[0m, in \u001b[0;36m_blend\u001b[0;34m(img1, img2, ratio)\u001b[0m\n\u001b[1;32m    309\u001b[0m ratio \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(ratio)\n\u001b[1;32m    310\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39mif\u001b[39;00m img1\u001b[39m.\u001b[39mis_floating_point() \u001b[39melse\u001b[39;00m \u001b[39m255.0\u001b[39m\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m (ratio \u001b[39m*\u001b[39;49m img1 \u001b[39m+\u001b[39;49m (\u001b[39m1.0\u001b[39;49m \u001b[39m-\u001b[39;49m ratio) \u001b[39m*\u001b[39;49m img2)\u001b[39m.\u001b[39mclamp(\u001b[39m0\u001b[39m, bound)\u001b[39m.\u001b[39mto(img1\u001b[39m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@skopt.utils.use_named_args(HYPERPARAM_SPACE)\n",
    "def val_neg_accuracy(**classifier_params):\n",
    "    classifier_params = dict(classifier_params)\n",
    "    \n",
    "    classifier = Classifier(vlm, **classifier_params)\n",
    "    \n",
    "    accuracies = []\n",
    "    for n_support in N_SUPPORT_LIST:\n",
    "        test_handler.run_few_shot_test(classifier, query_dataset, support_dataset, N_WAY, n_support, N_QUERY, N_EPISODES)\n",
    "\n",
    "        # Find dataframe row with accuracy from this run\n",
    "        filter_indices = np.ones(len(test_handler.results))\n",
    "        for key, val in dataframe_format(classifier, query_dataset, support_dataset, N_WAY, n_support, N_QUERY, N_EPISODES).items():\n",
    "            if pd.isna(val):\n",
    "                filter_indices = filter_indices & pd.isna(test_handler.results[key])\n",
    "            else:\n",
    "                filter_indices = filter_indices & (test_handler.results[key] == val)\n",
    "        matched_row = test_handler.results[filter_indices]\n",
    "        accuracies.append(matched_row[\"accuracy\"].values[0])\n",
    "        \n",
    "    return -1 * np.mean(accuracies)\n",
    "\n",
    "skopt_pbar = None\n",
    "def callback(current_skopt_results):\n",
    "    skopt.dump(current_skopt_results, SKOPT_RESULTS_FILE)\n",
    "    \n",
    "    best_run_ind = np.argmin(current_skopt_results.func_vals)\n",
    "    postfix_dict = {\n",
    "        \"best_acc\": round(-1 * current_skopt_results.func_vals[best_run_ind], 5)\n",
    "    }\n",
    "    postfix_dict.update({\n",
    "        param_space.name: round(current_skopt_results.x_iters[best_run_ind][i], 5)\n",
    "        for i, param_space in enumerate(HYPERPARAM_SPACE)\n",
    "    })\n",
    "    skopt_pbar.update(1)\n",
    "    skopt_pbar.set_postfix(postfix_dict)\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(SKOPT_RESULTS_FILE):\n",
    "    old_skopt_results = skopt.load(SKOPT_RESULTS_FILE)\n",
    "    x0 = old_skopt_results.x_iters\n",
    "    y0 = old_skopt_results.func_vals\n",
    "else:\n",
    "    x0 = None\n",
    "    y0 = None\n",
    "skopt_pbar = tqdm(total=SKOPT_N_CALLS) \n",
    "skopt_results = skopt.gp_minimize(val_neg_accuracy, HYPERPARAM_SPACE, n_calls=SKOPT_N_CALLS, callback=callback, x0=x0, y0=y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Hyperparam Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skopt_results = skopt.load(SKOPT_RESULTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr                   0.016751266149792866\n",
      "random_augment       0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "skopt.plots.plot_objective(skopt_results)\n",
    "for i, hyperparam in enumerate(HYPERPARAM_SPACE):\n",
    "    print(f\"{hyperparam.name:20} {skopt_results.x[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_SUPPORT_LIST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rob/vlm_benchmark/hyperparam_search.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m averaged_over_support \u001b[39m=\u001b[39m test_handler\u001b[39m.\u001b[39mresults\u001b[39m.\u001b[39mgroupby([col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m test_handler\u001b[39m.\u001b[39mresults\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mn_support\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maccuracy_std\u001b[39m\u001b[39m\"\u001b[39m]], as_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dropna\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Drop any sets which didn't complete the full list of support values (if interrupted, etc)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m averaged_over_support \u001b[39m=\u001b[39m averaged_over_support[averaged_over_support[\u001b[39m\"\u001b[39m\u001b[39mn_support\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mmean(N_SUPPORT_LIST)]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m hyperparam \u001b[39min\u001b[39;00m HYPERPARAM_SPACE:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b227d/home/rob/vlm_benchmark/hyperparam_search.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     plt\u001b[39m.\u001b[39mscatter(averaged_over_support[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclassifier.\u001b[39m\u001b[39m{\u001b[39;00mhyperparam\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues, averaged_over_support[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N_SUPPORT_LIST' is not defined"
     ]
    }
   ],
   "source": [
    "averaged_over_support = test_handler.results.groupby([col for col in test_handler.results.columns if col not in [\"n_support\", \"accuracy\", \"accuracy_std\"]], as_index=False, dropna=False).mean()\n",
    "\n",
    "# Drop any sets which didn't complete the full list of support values (if interrupted, etc)\n",
    "averaged_over_support = averaged_over_support[averaged_over_support[\"n_support\"] == np.mean(N_SUPPORT_LIST)]\n",
    "\n",
    "for hyperparam in HYPERPARAM_SPACE:\n",
    "    plt.scatter(averaged_over_support[f\"classifier.{hyperparam.name}\"].values, averaged_over_support[\"accuracy\"].values)\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(hyperparam.name)\n",
    "    plt.title(hyperparam.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('videoclip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "576597d045da71b18680487735a015f28433d9aa438b3c061008c825d6c37722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
