{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2703466/3130838648.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import importlib\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "from FewShotTestHandler import FewShotTestHandler, optimize_hyperparameters, find_hyperparameters, test_already_stored\n",
    "from dataset import DatasetHandler\n",
    "from similarity_metrics import Similarity\n",
    "from plotting_utils import plot\n",
    "\n",
    "ENV = os.environ[\"CONDA_DEFAULT_ENV\"]\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {}\n",
    "\n",
    "# Dataset Params - dataset.____ keys are passed into DatasetHandler constructor\n",
    "params_dict[\"dataset.name\"] = [\"smsm\", \"kinetics_100\", \"moma_act\", \"moma_sact\"]\n",
    "params_dict[\"dataset.split\"] = [\"val\"]\n",
    "params_dict[\"dataset.split_type\"] = [\"video\"]\n",
    "\n",
    "# Few-Shot Test Params - test.____ keys are passed into few-shot test call\n",
    "params_dict[\"test.n_way\"] = [None] # None gets converted into the max value for each dataset\n",
    "params_dict[\"test.n_support\"] = [0, 1, 2, 4, 8, 16]\n",
    "params_dict[\"test.n_query\"] = [None]\n",
    "params_dict[\"test.n_episodes\"] = [4]\n",
    "\n",
    "# VLM Params - vlm.____ keys are passed into VLM constructor\n",
    "if ENV == \"VLM_CLIP\":\n",
    "    from CLIP.CLIPVLM import ClipVLM as VLM\n",
    "    params_dict[\"vlm.num_frames\"] = [10]\n",
    "elif ENV == \"VLM_MILES\":\n",
    "    from MILES.wrapper import MILES_SimilarityVLM as VLM\n",
    "elif ENV == \"videoclip\":\n",
    "    from video_clip.video_clip import VideoClipVLM as VLM\n",
    "    params_dict[\"vlm.num_seconds\"] = [4]\n",
    "    params_dict[\"vlm.sample_strat\"] = [\"spread\"]\n",
    "    params_dict[\"vlm.use_cuda\"] = [True]\n",
    "elif ENV == \"VLM_UNIVL\":\n",
    "    from UNIVL.wrapper import UniVL_SimilarityVLM as VLM\n",
    "elif ENV == \"VLM_VTTWINS\":\n",
    "    from VTTWINS.wrapper import VTTWINS_SimilarityVLM as VLM\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Classifier Params - classifier.____ keys are passed into classifier constructor\n",
    "if False:\n",
    "    from classifier import WeightedTextFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 4.0, 10.0, 100.0]\n",
    "    #params_dict[\"classifier.metric\"] = [Similarity.COSINE, Similarity.DOT, Similarity.EUCLID]\n",
    "if True:\n",
    "    from classifier import HardPromptFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 2.0, 4.0, 6.0, 8.0, 10.0, 20.0, 30.0, 40.0, 50.0, 100.0]\n",
    "    params_dict[\"classifier.prompt_text\"] = [\n",
    "        \"\",\n",
    "        \"a photo showing the task of\",\n",
    "        \"an activity of\",\n",
    "        \"the video shows me\"\n",
    "    ]\n",
    "if False:\n",
    "    from classifier import NearestNeighborFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.neighbor_count\"] = [1, 2, 3, 4, 5, 10, 20]\n",
    "    params_dict[\"classifier.neighbor_weights\"] = [\"uniform\", \"distance\"]\n",
    "if False:\n",
    "    from classifier import GaussianFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 4.0, 10.0, 100.0]\n",
    "    params_dict[\"classifier.prior_count\"] = [0, 1, 3, 10, 30, 100]\n",
    "    params_dict[\"classifier.prior_var\"] = [0, 1, 3, 10, 30, 100]\n",
    "if False:\n",
    "    from classifier import SubVideoAverageFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 4.0, 10.0, 100.0]\n",
    "    params_dict[\"classifier.subvideo_segment_duration\"] = [1, 2, 5]\n",
    "    params_dict[\"classifier.subvideo_max_segments\"] = [32]\n",
    "    params_dict[\"classifier.subvideo_discard_proportion\"] = [0, 0.1, 0.25, 0.5]\n",
    "if False:\n",
    "    from classifier import TipAdapterFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.alpha\"] = [858]#[238]#[100, 120, 140] #[0.5, 1.0, 2.0]\n",
    "    params_dict[\"classifier.beta\"] = [26]#[5.07] #[2.5, 5.5, 10.0]\n",
    "    params_dict[\"classifier.finetune_lr\"] = [0]#[5.9e-4]#[1e-4, 3e-4, 1e-3, 3e-3, 1e-2]\n",
    "    params_dict[\"classifier.finetune_epochs\"] = [0]#[10] #[0, 1, 5, 10, 20]\n",
    "    params_dict[\"classifier.weight_decay\"] = [0.0001]\n",
    "if False:\n",
    "    from classifier.smsm_object_oracle import SmsmObjectOracleFewShotClassifier as Classifier\n",
    "if False:\n",
    "    from classifier.coop import CoopFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.lr\"] = [2e-4]#, 2e-3, 1e-2]\n",
    "    params_dict[\"classifier.epochs\"] = [10]\n",
    "    params_dict[\"classifier.warmup_epochs\"] = [1]\n",
    "    params_dict[\"classifier.random_augment\"] = [True]\n",
    "    params_dict[\"classifier.batch_size\"] = [8]\n",
    "if False:\n",
    "    from classifier.cona import CoNaFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.batch_size\"] = [8]\n",
    "    params_dict[\"classifier.random_augment\"] = [True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILENAME = \"vl_proto.csv\"\n",
    "test_handler = FewShotTestHandler(TEST_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85edbe70cfae40a5af73c7922978a589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH IS: video_clip/MMPT_updated/projects/retri/videoclip/how2.yaml\n",
      "CKPT SAVE DIR: video_clip/MMPT_updated/runs/retri/videoclip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MMBertForEncoder: ['bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'cls.predictions.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.8.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight']\n",
      "- This IS expected if you are initializing MMBertForEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MMBertForEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MMBertForEncoder were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['videomlp.linear2.weight', 'videomlp.linear2.bias', 'videomlp.linear1.bias', 'videomlp.linear1.weight', 'videomlp.LayerNorm.bias', 'videomlp.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c98ee69dbc4108a2070045e649eb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd87e27326d545ada7df29523b6e156c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e8ca18061f462eb4a3adf2c6d826ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2407f1e4a2942d6b8b38d878e753822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8007dbba6da3451498c963a6d739cde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882d6a070efa4aaa9449cc9a8bbce378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391fd6e9f3d2446aaa389631c5c81184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f5e35b6d7e4f1284e1a0fbd69d4b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a67b25724f74da88d000c55c3f1a434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67749842cd0c4b42815aa2c7970a3293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca00e1197c344c4dad13b89314246945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d246515e6ff7466a8320001874bd9fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15918a8749447feba91d3ef2801e0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d576c10e9b714ee9acc8b1a6cc2137cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1ea6a183c042bbad9eec37963139cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2446ca50b9430791e991fbafec9e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce68a7d53014f08bba30abafa2ddc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bfaf68fb3048e381cc5f993301a939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af5379a41214806ba69e44740fb3c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6aedaf98bab47afa73363936731fc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036f875cb77e4402b6fe8f005ad052e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c7562320ef41ac806bd0163576c218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5602cd51150a4e4b882e310898ea238d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c4074a5f1c4436ae2a366b9fc1d413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0441ed60c19b4f0a805c3697ecdec1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10a941bcb684daa8b0de5f69bc5f4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9c2bd535f045288c1566b37f4af4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311c09c48d7147af9561ea3bd419dc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fa9579c5674170961137bffd8f9ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91bd80841f3420d95464190c4ce1bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5bdc18a4734dd9b9b9c7edc3414fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31cf82d8b644cc9b4a2c8ac16e1a705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165fa76dfcfc402b9b79fcac64693b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1274d7cdf9b04634b9a497cda2324ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ee6d830b7c455aae2a6f58cceea6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6241cd2e6def4ddb83fab61b2997ca40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab771cf9f7e441594c64e7d9b3c7b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c14b0367aa8420fbde3ce57a43bbab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785ff7dabee144cbad7a720f3f3cbd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b35c846f16f487d8ef02995d96e3667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51999360a123467493d525856c047561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vlm = None\n",
    "cur_vlm_params = None\n",
    "classifier = None\n",
    "cur_classifier_params = None\n",
    "query_dataset = None\n",
    "support_dataset = None\n",
    "cur_dataset_params = None\n",
    "\n",
    "pbar = tqdm(list(itertools.product(*params_dict.values())))\n",
    "for params in pbar:\n",
    "    # Associate keys to each param\n",
    "    params = dict(zip(params_dict.keys(), params))\n",
    "    \n",
    "    pbar.set_postfix(params)\n",
    "    \n",
    "    # vlm params\n",
    "    vlm_params = {key[4:]: val for key, val in params.items() if key.startswith(\"vlm.\")}\n",
    "    classifier_params = {key[11:]: val for key, val in params.items() if key.startswith(\"classifier.\")}\n",
    "    dataset_params = {key[8:]: val for key, val in params.items() if key.startswith(\"dataset.\")}\n",
    "    test_params = {key[5:]: val for key, val in params.items() if key.startswith(\"test.\")}\n",
    "    \n",
    "    # Update dataset\n",
    "    if query_dataset is None or cur_dataset_params != dataset_params:\n",
    "        query_dataset = DatasetHandler(**dataset_params)\n",
    "        support_dataset_params = dict(dataset_params, split=\"train\")\n",
    "        support_dataset = DatasetHandler(**support_dataset_params)\n",
    "        \n",
    "        cur_dataset_params = dataset_params\n",
    "        new_dataset = True\n",
    "    else:\n",
    "        new_dataset = False\n",
    "        \n",
    "    # Convert n_way = None into n_way = max-ways\n",
    "    if test_params[\"n_way\"] is None:\n",
    "        test_params[\"n_way\"] = support_dataset.category_count()\n",
    "    \n",
    "    # Update vlm (which forces update of classifier)\n",
    "    if vlm is None or cur_vlm_params != vlm_params:\n",
    "        vlm = VLM(**vlm_params)\n",
    "        \n",
    "        cur_vlm_params = vlm_params\n",
    "        new_vlm = True\n",
    "    else:\n",
    "        new_vlm = False\n",
    "            \n",
    "    if new_vlm or classifier is None or cur_classifier_params != classifier_params:\n",
    "        classifier = Classifier(vlm, **classifier_params)\n",
    "        cur_classifier_params = classifier_params\n",
    "        \n",
    "    # Fill dataset caches\n",
    "    if new_dataset or new_vlm:\n",
    "        query_dataset.fill_cache(vlm)\n",
    "        support_dataset.fill_cache(vlm)\n",
    "    \n",
    "    # Run test\n",
    "    test_handler.run_few_shot_test(classifier, query_dataset, support_dataset, **test_params)\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vlm_class</th>\n",
       "      <th>vlm.num_seconds</th>\n",
       "      <th>vlm.path</th>\n",
       "      <th>vlm.sample_strat</th>\n",
       "      <th>vlm.use_cuda</th>\n",
       "      <th>classifier_class</th>\n",
       "      <th>classifier.metric</th>\n",
       "      <th>classifier.prompt_location</th>\n",
       "      <th>classifier.prompt_text</th>\n",
       "      <th>classifier.text_weight</th>\n",
       "      <th>query_dataset</th>\n",
       "      <th>support_dataset</th>\n",
       "      <th>n_way</th>\n",
       "      <th>n_support</th>\n",
       "      <th>n_query</th>\n",
       "      <th>n_episodes</th>\n",
       "      <th>val_tuning_dataset</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050227</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>0.0</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050227</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>an activity of</td>\n",
       "      <td>0.0</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050227</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>the video shows me</td>\n",
       "      <td>0.0</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050227</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051591</td>\n",
       "      <td>0.007589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>the video shows me</td>\n",
       "      <td>50.0</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.323681</td>\n",
       "      <td>0.011805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td></td>\n",
       "      <td>100.0</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.295011</td>\n",
       "      <td>0.008472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>100.0</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.281823</td>\n",
       "      <td>0.007240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>an activity of</td>\n",
       "      <td>100.0</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.282110</td>\n",
       "      <td>0.004690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>the video shows me</td>\n",
       "      <td>100.0</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.300937</td>\n",
       "      <td>0.005640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1269 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         vlm_class  vlm.num_seconds  \\\n",
       "0     VideoClipVLM                4   \n",
       "1     VideoClipVLM                4   \n",
       "2     VideoClipVLM                4   \n",
       "3     VideoClipVLM                4   \n",
       "4     VideoClipVLM                4   \n",
       "...            ...              ...   \n",
       "1264  VideoClipVLM                4   \n",
       "1265  VideoClipVLM                4   \n",
       "1266  VideoClipVLM                4   \n",
       "1267  VideoClipVLM                4   \n",
       "1268  VideoClipVLM                4   \n",
       "\n",
       "                                               vlm.path vlm.sample_strat  \\\n",
       "0     video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "1     video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "2     video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "3     video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "4     video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "...                                                 ...              ...   \n",
       "1264  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "1265  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "1266  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "1267  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "1268  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "\n",
       "      vlm.use_cuda             classifier_class classifier.metric  \\\n",
       "0             True  HardPromptFewShotClassifier               DOT   \n",
       "1             True  HardPromptFewShotClassifier               DOT   \n",
       "2             True  HardPromptFewShotClassifier               DOT   \n",
       "3             True  HardPromptFewShotClassifier               DOT   \n",
       "4             True  HardPromptFewShotClassifier               DOT   \n",
       "...            ...                          ...               ...   \n",
       "1264          True  HardPromptFewShotClassifier               DOT   \n",
       "1265          True  HardPromptFewShotClassifier               DOT   \n",
       "1266          True  HardPromptFewShotClassifier               DOT   \n",
       "1267          True  HardPromptFewShotClassifier               DOT   \n",
       "1268          True  HardPromptFewShotClassifier               DOT   \n",
       "\n",
       "     classifier.prompt_location       classifier.prompt_text  \\\n",
       "0                         start                          NaN   \n",
       "1                         start  a photo showing the task of   \n",
       "2                         start               an activity of   \n",
       "3                         start           the video shows me   \n",
       "4                         start                          NaN   \n",
       "...                         ...                          ...   \n",
       "1264                      start           the video shows me   \n",
       "1265                      start                                \n",
       "1266                      start  a photo showing the task of   \n",
       "1267                      start               an activity of   \n",
       "1268                      start           the video shows me   \n",
       "\n",
       "      classifier.text_weight    query_dataset    support_dataset  n_way  \\\n",
       "0                        0.0       smsm.v.val       smsm.v.train    100   \n",
       "1                        0.0       smsm.v.val       smsm.v.train    100   \n",
       "2                        0.0       smsm.v.val       smsm.v.train    100   \n",
       "3                        0.0       smsm.v.val       smsm.v.train    100   \n",
       "4                        0.1       smsm.v.val       smsm.v.train    100   \n",
       "...                      ...              ...                ...    ...   \n",
       "1264                    50.0  moma_sact.v.val  moma_sact.v.train     80   \n",
       "1265                   100.0  moma_sact.v.val  moma_sact.v.train     80   \n",
       "1266                   100.0  moma_sact.v.val  moma_sact.v.train     80   \n",
       "1267                   100.0  moma_sact.v.val  moma_sact.v.train     80   \n",
       "1268                   100.0  moma_sact.v.val  moma_sact.v.train     80   \n",
       "\n",
       "      n_support  n_query  n_episodes  val_tuning_dataset  accuracy  \\\n",
       "0             1      NaN           4                 NaN  0.050227   \n",
       "1             1      NaN           4                 NaN  0.050227   \n",
       "2             1      NaN           4                 NaN  0.050227   \n",
       "3             1      NaN           4                 NaN  0.050227   \n",
       "4             1      NaN           4                 NaN  0.051591   \n",
       "...         ...      ...         ...                 ...       ...   \n",
       "1264         16      NaN           4                 NaN  0.323681   \n",
       "1265         16      NaN           4                 NaN  0.295011   \n",
       "1266         16      NaN           4                 NaN  0.281823   \n",
       "1267         16      NaN           4                 NaN  0.282110   \n",
       "1268         16      NaN           4                 NaN  0.300937   \n",
       "\n",
       "      accuracy_std  \n",
       "0         0.006599  \n",
       "1         0.006599  \n",
       "2         0.006599  \n",
       "3         0.006599  \n",
       "4         0.007589  \n",
       "...            ...  \n",
       "1264      0.011805  \n",
       "1265      0.008472  \n",
       "1266      0.007240  \n",
       "1267      0.004690  \n",
       "1268      0.005640  \n",
       "\n",
       "[1269 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_handler.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Best Hyperparameters on Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vlm_class</th>\n",
       "      <th>classifier_class</th>\n",
       "      <th>query_dataset</th>\n",
       "      <th>support_dataset</th>\n",
       "      <th>n_way</th>\n",
       "      <th>n_support</th>\n",
       "      <th>n_query</th>\n",
       "      <th>n_episodes</th>\n",
       "      <th>val_tuning_dataset</th>\n",
       "      <th>vlm.num_seconds</th>\n",
       "      <th>vlm.path</th>\n",
       "      <th>vlm.sample_strat</th>\n",
       "      <th>vlm.use_cuda</th>\n",
       "      <th>classifier.metric</th>\n",
       "      <th>classifier.prompt_location</th>\n",
       "      <th>classifier.prompt_text</th>\n",
       "      <th>classifier.text_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_act.v.val</td>\n",
       "      <td>moma_act.v.train</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_act.v.val</td>\n",
       "      <td>moma_act.v.train</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_act.v.val</td>\n",
       "      <td>moma_act.v.train</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_act.v.val</td>\n",
       "      <td>moma_act.v.train</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>the video shows me</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_act.v.val</td>\n",
       "      <td>moma_act.v.train</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_act.v.val</td>\n",
       "      <td>moma_act.v.train</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>kinetics_100.v.val</td>\n",
       "      <td>kinetics_100.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>kinetics_100.v.val</td>\n",
       "      <td>kinetics_100.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>the video shows me</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>kinetics_100.v.val</td>\n",
       "      <td>kinetics_100.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>kinetics_100.v.val</td>\n",
       "      <td>kinetics_100.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>the video shows me</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>kinetics_100.v.val</td>\n",
       "      <td>kinetics_100.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td></td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>kinetics_100.v.val</td>\n",
       "      <td>kinetics_100.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>a photo showing the task of</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>the video shows me</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>the video shows me</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td></td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>moma_sact.v.val</td>\n",
       "      <td>moma_sact.v.train</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td></td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>an activity of</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>an activity of</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td></td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>the video shows me</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td></td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>HardPromptFewShotClassifier</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>DOT</td>\n",
       "      <td>start</td>\n",
       "      <td>the video shows me</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         vlm_class             classifier_class       query_dataset  \\\n",
       "593   VideoClipVLM  HardPromptFewShotClassifier      moma_act.v.val   \n",
       "541   VideoClipVLM  HardPromptFewShotClassifier      moma_act.v.val   \n",
       "434   VideoClipVLM  HardPromptFewShotClassifier      moma_act.v.val   \n",
       "514   VideoClipVLM  HardPromptFewShotClassifier      moma_act.v.val   \n",
       "384   VideoClipVLM  HardPromptFewShotClassifier      moma_act.v.val   \n",
       "334   VideoClipVLM  HardPromptFewShotClassifier      moma_act.v.val   \n",
       "281   VideoClipVLM  HardPromptFewShotClassifier  kinetics_100.v.val   \n",
       "253   VideoClipVLM  HardPromptFewShotClassifier  kinetics_100.v.val   \n",
       "175   VideoClipVLM  HardPromptFewShotClassifier  kinetics_100.v.val   \n",
       "148   VideoClipVLM  HardPromptFewShotClassifier  kinetics_100.v.val   \n",
       "69    VideoClipVLM  HardPromptFewShotClassifier  kinetics_100.v.val   \n",
       "889   VideoClipVLM  HardPromptFewShotClassifier     moma_sact.v.val   \n",
       "18    VideoClipVLM  HardPromptFewShotClassifier  kinetics_100.v.val   \n",
       "823   VideoClipVLM  HardPromptFewShotClassifier     moma_sact.v.val   \n",
       "839   VideoClipVLM  HardPromptFewShotClassifier     moma_sact.v.val   \n",
       "773   VideoClipVLM  HardPromptFewShotClassifier     moma_sact.v.val   \n",
       "679   VideoClipVLM  HardPromptFewShotClassifier     moma_sact.v.val   \n",
       "634   VideoClipVLM  HardPromptFewShotClassifier     moma_sact.v.val   \n",
       "1251  VideoClipVLM  HardPromptFewShotClassifier          smsm.v.val   \n",
       "1198  VideoClipVLM  HardPromptFewShotClassifier          smsm.v.val   \n",
       "1119  VideoClipVLM  HardPromptFewShotClassifier          smsm.v.val   \n",
       "1097  VideoClipVLM  HardPromptFewShotClassifier          smsm.v.val   \n",
       "994   VideoClipVLM  HardPromptFewShotClassifier          smsm.v.val   \n",
       "979   VideoClipVLM  HardPromptFewShotClassifier          smsm.v.val   \n",
       "\n",
       "           support_dataset  n_way  n_support  n_query  n_episodes  \\\n",
       "593       moma_act.v.train     19         16      NaN           4   \n",
       "541       moma_act.v.train     19          8      NaN           4   \n",
       "434       moma_act.v.train     19          2      NaN           4   \n",
       "514       moma_act.v.train     19          4      NaN           4   \n",
       "384       moma_act.v.train     19          1      NaN           4   \n",
       "334       moma_act.v.train     19          0      NaN           4   \n",
       "281   kinetics_100.v.train    100         16      NaN           4   \n",
       "253   kinetics_100.v.train    100          8      NaN           4   \n",
       "175   kinetics_100.v.train    100          4      NaN           4   \n",
       "148   kinetics_100.v.train    100          2      NaN           4   \n",
       "69    kinetics_100.v.train    100          1      NaN           4   \n",
       "889      moma_sact.v.train     80         16      NaN           4   \n",
       "18    kinetics_100.v.train    100          0      NaN           4   \n",
       "823      moma_sact.v.train     80          4      NaN           4   \n",
       "839      moma_sact.v.train     80          8      NaN           4   \n",
       "773      moma_sact.v.train     80          2      NaN           4   \n",
       "679      moma_sact.v.train     80          1      NaN           4   \n",
       "634      moma_sact.v.train     80          0      NaN           4   \n",
       "1251          smsm.v.train    100         16      NaN           4   \n",
       "1198          smsm.v.train    100          8      NaN           4   \n",
       "1119          smsm.v.train    100          4      NaN           4   \n",
       "1097          smsm.v.train    100          2      NaN           4   \n",
       "994           smsm.v.train    100          1      NaN           4   \n",
       "979           smsm.v.train    100          0      NaN           4   \n",
       "\n",
       "      val_tuning_dataset  vlm.num_seconds  \\\n",
       "593                  NaN                4   \n",
       "541                  NaN                4   \n",
       "434                  NaN                4   \n",
       "514                  NaN                4   \n",
       "384                  NaN                4   \n",
       "334                  NaN                4   \n",
       "281                  NaN                4   \n",
       "253                  NaN                4   \n",
       "175                  NaN                4   \n",
       "148                  NaN                4   \n",
       "69                   NaN                4   \n",
       "889                  NaN                4   \n",
       "18                   NaN                4   \n",
       "823                  NaN                4   \n",
       "839                  NaN                4   \n",
       "773                  NaN                4   \n",
       "679                  NaN                4   \n",
       "634                  NaN                4   \n",
       "1251                 NaN                4   \n",
       "1198                 NaN                4   \n",
       "1119                 NaN                4   \n",
       "1097                 NaN                4   \n",
       "994                  NaN                4   \n",
       "979                  NaN                4   \n",
       "\n",
       "                                               vlm.path vlm.sample_strat  \\\n",
       "593   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "541   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "434   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "514   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "384   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "334   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "281   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "253   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "175   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "148   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "69    video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "889   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "18    video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "823   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "839   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "773   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "679   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "634   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "1251  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "1198  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "1119  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "1097  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "994   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "979   video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "\n",
       "      vlm.use_cuda classifier.metric classifier.prompt_location  \\\n",
       "593           True               DOT                      start   \n",
       "541           True               DOT                      start   \n",
       "434           True               DOT                      start   \n",
       "514           True               DOT                      start   \n",
       "384           True               DOT                      start   \n",
       "334           True               DOT                      start   \n",
       "281           True               DOT                      start   \n",
       "253           True               DOT                      start   \n",
       "175           True               DOT                      start   \n",
       "148           True               DOT                      start   \n",
       "69            True               DOT                      start   \n",
       "889           True               DOT                      start   \n",
       "18            True               DOT                      start   \n",
       "823           True               DOT                      start   \n",
       "839           True               DOT                      start   \n",
       "773           True               DOT                      start   \n",
       "679           True               DOT                      start   \n",
       "634           True               DOT                      start   \n",
       "1251          True               DOT                      start   \n",
       "1198          True               DOT                      start   \n",
       "1119          True               DOT                      start   \n",
       "1097          True               DOT                      start   \n",
       "994           True               DOT                      start   \n",
       "979           True               DOT                      start   \n",
       "\n",
       "           classifier.prompt_text  classifier.text_weight  \n",
       "593   a photo showing the task of                    20.0  \n",
       "541   a photo showing the task of                    20.0  \n",
       "434   a photo showing the task of                     6.0  \n",
       "514            the video shows me                    10.0  \n",
       "384   a photo showing the task of                    10.0  \n",
       "334   a photo showing the task of                    30.0  \n",
       "281   a photo showing the task of                    20.0  \n",
       "253            the video shows me                     8.0  \n",
       "175   a photo showing the task of                     8.0  \n",
       "148            the video shows me                     6.0  \n",
       "69    a photo showing the task of                     4.0  \n",
       "889                                                   6.0  \n",
       "18    a photo showing the task of                     6.0  \n",
       "823            the video shows me                     4.0  \n",
       "839                                                  10.0  \n",
       "773            the video shows me                     8.0  \n",
       "679                                                   2.0  \n",
       "634                                                  40.0  \n",
       "1251               an activity of                    20.0  \n",
       "1198               an activity of                    10.0  \n",
       "1119                                                  8.0  \n",
       "1097           the video shows me                     6.0  \n",
       "994                                                   8.0  \n",
       "979            the video shows me                     4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_hyperparam_values = find_hyperparameters(\n",
    "    test_handler.results,\n",
    "    hyperparam_cols=[col for col in test_handler.results if col.startswith(\"classifier.\") or col.startswith(\"vlm.\")]\n",
    ")\n",
    "display(best_hyperparam_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab373ea564d9436091b02ce53df7ef44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH IS: video_clip/MMPT_updated/projects/retri/videoclip/how2.yaml\n",
      "CKPT SAVE DIR: video_clip/MMPT_updated/runs/retri/videoclip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MMBertForEncoder: ['bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'cls.predictions.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing MMBertForEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MMBertForEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MMBertForEncoder were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['videomlp.linear1.weight', 'videomlp.linear1.bias', 'videomlp.LayerNorm.weight', 'videomlp.linear2.bias', 'videomlp.LayerNorm.bias', 'videomlp.linear2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 678.00 MiB (GPU 0; 15.78 GiB total capacity; 2.56 GiB already allocated; 440.25 MiB free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/rob/vlm_benchmark/vlm_param_tests.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# Update vlm (which forces update of classifier)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mif\u001b[39;00m vlm \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m cur_vlm_params \u001b[39m!=\u001b[39m vlm_params:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     vlm \u001b[39m=\u001b[39m VLM(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvlm_params)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     cur_vlm_params \u001b[39m=\u001b[39m vlm_params\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m     new_vlm \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/vlm_benchmark/video_clip/video_clip.py:66\u001b[0m, in \u001b[0;36mVideoClipVLM.__init__\u001b[0;34m(self, path, num_seconds, sample_strat, use_cuda, reset_cache)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_strat \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mspread\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     65\u001b[0m \u001b[39m# Load model\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_model(path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath)\n\u001b[1;32m     68\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(cache_file\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(FILE_DIR, CACHE_NAME), reset_cache\u001b[39m=\u001b[39mreset_cache)\n",
      "File \u001b[0;32m~/vlm_benchmark/video_clip/video_clip.py:108\u001b[0m, in \u001b[0;36mVideoClipVLM.load_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCKPT SAVE DIR:\u001b[39m\u001b[39m\"\u001b[39m, ckpt_save_dir) \u001b[39m# /home/zaned/code/vlm_benchmark/video_clip/MMPT_updated/projects/retri/videoclip\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m# Target: /home/zaned/code/vlm_benchmark/video_clip/MMPT_updated/runs/retri/videoclip/checkpoint_best.pt\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m MMPTClassifier\u001b[39m.\u001b[39;49mfrom_pretrained(path, embed_extractor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ckpt_save_dir\u001b[39m=\u001b[39;49mckpt_save_dir,\n\u001b[1;32m    109\u001b[0m                                             use_cuda\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcuda)\n\u001b[1;32m    111\u001b[0m \u001b[39m# Load random caps/cmasks for VideoCLIP so that video embeddings can be run without\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m# needing to extract text embeddings first.  VideoCLIP requires both text and video inputs\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m# at inference time, but uses attention mechanisms to prevent cross-modal leakage. We abstract\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m# this away here.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m random_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrandom text\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/vlm_benchmark/video_clip/MMPT_updated/mmpt/models/mmfusion.py:109\u001b[0m, in \u001b[0;36mMMPTClassifier.from_pretrained\u001b[0;34m(cls, config, checkpoint, embed_extractor, ckpt_save_dir, use_cuda)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     checkpoint_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ckpt_save_dir, checkpoint)\n\u001b[0;32m--> 109\u001b[0m mmtask\u001b[39m.\u001b[39;49mbuild_model(checkpoint\u001b[39m=\u001b[39;49mcheckpoint_path, use_cuda\u001b[39m=\u001b[39;49muse_cuda)\n\u001b[1;32m    110\u001b[0m \u001b[39m# TODO(huxu): make the video encoder configurable.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprocessors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39ms3dg\u001b[39;00m \u001b[39mimport\u001b[39;00m S3D\n",
      "File \u001b[0;32m~/vlm_benchmark/video_clip/MMPT_updated/mmpt/tasks/task.py:101\u001b[0m, in \u001b[0;36mTask.build_model\u001b[0;34m(self, checkpoint, use_cuda)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig)\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m checkpoint \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_checkpoint(checkpoint, use_cuda)\n\u001b[1;32m    102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n",
      "File \u001b[0;32m~/vlm_benchmark/video_clip/MMPT_updated/mmpt/tasks/task.py:108\u001b[0m, in \u001b[0;36mTask.load_checkpoint\u001b[0;34m(self, checkpoint, use_cuda)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodel is not initialized.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m use_cuda:\n\u001b[0;32m--> 108\u001b[0m     state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(checkpoint)\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(checkpoint, map_location\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_UntypedStorage(obj\u001b[39m.\u001b[39mnbytes(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(location))\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/_utils.py:78\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m new_type(indices, values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_UntypedStorage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 678.00 MiB (GPU 0; 15.78 GiB total capacity; 2.56 GiB already allocated; 440.25 MiB free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Change params_dict to only include dataset and test info, then run tests with best hyperparameter values\n",
    "test_split_params_dict = {\n",
    "    \"dataset.split\": [\"test\"]\n",
    "}\n",
    "for key, val in params_dict.items():\n",
    "    if key.startswith(\"classifier.\") or key.startswith(\"vlm.\") or key == \"dataset.split\":\n",
    "        continue\n",
    "    test_split_params_dict[key] = val\n",
    "    \n",
    "final_test_handler = FewShotTestHandler(f\"test.{TEST_FILENAME}\")\n",
    "    \n",
    "vlm = None\n",
    "cur_vlm_params = None\n",
    "classifier = None\n",
    "cur_classifier_params = None\n",
    "query_dataset = None\n",
    "support_dataset = None\n",
    "cur_dataset_params = None\n",
    "\n",
    "pbar = tqdm(list(itertools.product(*test_split_params_dict.values())))\n",
    "for params in pbar:\n",
    "    # Associate keys to each param\n",
    "    params = dict(zip(test_split_params_dict.keys(), params))\n",
    "    \n",
    "    # Determine dataset and test parameters\n",
    "    dataset_params = {key[8:]: val for key, val in params.items() if key.startswith(\"dataset.\")}\n",
    "    test_params = {key[5:]: val for key, val in params.items() if key.startswith(\"test.\")}\n",
    "    \n",
    "    # Update dataset\n",
    "    if query_dataset is None or cur_dataset_params != dataset_params:\n",
    "        query_dataset = DatasetHandler(**dataset_params)\n",
    "        support_dataset_params = dict(dataset_params, split=\"train\")\n",
    "        support_dataset = DatasetHandler(**support_dataset_params)\n",
    "        \n",
    "        # Construct dummy val dataset to get id for filtering dataframe results with corresponding hyperparameters\n",
    "        val_dataset = DatasetHandler(**dict(dataset_params, split=\"val\"))\n",
    "        \n",
    "        cur_dataset_params = dataset_params\n",
    "        new_dataset = True\n",
    "    else:\n",
    "        new_dataset = False\n",
    "        \n",
    "    # Convert n_way = None into n_way = max-ways\n",
    "    if test_params[\"n_way\"] is None:\n",
    "        test_params[\"n_way\"] = support_dataset.category_count()\n",
    "        \n",
    "    # Determine vlm and classifier params from hyperparameter dataframe\n",
    "    matched_hyperparam_values = np.ones(len(best_hyperparam_values)).astype(bool)\n",
    "    matched_hyperparam_values &= (best_hyperparam_values[\"vlm_class\"] == VLM.__name__) & (best_hyperparam_values[\"classifier_class\"] == Classifier.__name__)\n",
    "    matched_hyperparam_values &= (best_hyperparam_values[\"query_dataset\"] == val_dataset.id()) & (best_hyperparam_values[\"support_dataset\"] == support_dataset.id())\n",
    "    for col, val in test_params.items():\n",
    "        if pd.isna(val):\n",
    "            matched_hyperparam_values &= pd.isna(best_hyperparam_values[col])        \n",
    "        else:\n",
    "            matched_hyperparam_values &= (best_hyperparam_values[col] == val)\n",
    "    matched_hyperparam_values = best_hyperparam_values[matched_hyperparam_values].reset_index(drop=True)\n",
    "    \n",
    "    vlm_params = {}\n",
    "    classifier_params = {}\n",
    "    for col in matched_hyperparam_values.columns:\n",
    "        if col.startswith(\"vlm.\"):\n",
    "            val = matched_hyperparam_values.loc[0, col]\n",
    "            if not pd.isna(val):\n",
    "                vlm_params[col[4:]] = val\n",
    "        \n",
    "        if col.startswith(\"classifier.\"):\n",
    "            val = matched_hyperparam_values.loc[0, col]\n",
    "            if not pd.isna(val):\n",
    "                if col != \"classifier.metric\":\n",
    "                    classifier_params[col[11:]] = val\n",
    "                else:\n",
    "                    classifier_params[col[11:]] = Similarity[val]\n",
    "        \n",
    "    for key, val in vlm_params.items():\n",
    "        params[f\"vlm.{key}\"] = val\n",
    "    for key, val in classifier_params.items():\n",
    "        params[f\"classifier.{key}\"] = val\n",
    "    pbar.set_postfix(params)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update vlm (which forces update of classifier)\n",
    "    if vlm is None or cur_vlm_params != vlm_params:\n",
    "        vlm = VLM(**vlm_params)\n",
    "        \n",
    "        cur_vlm_params = vlm_params\n",
    "        new_vlm = True\n",
    "    else:\n",
    "        new_vlm = False\n",
    "            \n",
    "    if new_vlm or classifier is None or cur_classifier_params != classifier_params:\n",
    "        classifier = Classifier(vlm, **classifier_params)\n",
    "        cur_classifier_params = classifier_params\n",
    "        \n",
    "    # Fill dataset caches\n",
    "    if new_dataset or new_vlm:\n",
    "        query_dataset.fill_cache(vlm)\n",
    "        support_dataset.fill_cache(vlm)\n",
    "    \n",
    "    # Run test\n",
    "    final_test_handler.run_few_shot_test(classifier, query_dataset, support_dataset, **test_params)\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_test_handler.results.sort_values([\"vlm_class\", \"classifier_class\", \"query_dataset\", \"n_support\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    final_test_handler,\n",
    "    x_col=\"n_support\",\n",
    "    y_col=\"accuracy\",\n",
    "    plot_descriptor_cols=[\"dataset\", \"classifier_class\"],\n",
    "    line_descriptor_cols=[\"vlm_class\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('videoclip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "576597d045da71b18680487735a015f28433d9aa438b3c061008c825d6c37722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
