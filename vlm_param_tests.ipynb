{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32023/1351999116.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "from FewShotTestHandler import FewShotTestHandler, optimize_hyperparameters, find_hyperparameters, test_already_stored\n",
    "from dataset import DatasetHandler\n",
    "from similarity_metrics import Similarity\n",
    "from plotting_utils import plot\n",
    "\n",
    "ENV = os.environ[\"CONDA_DEFAULT_ENV\"]\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_VAL_TUNING = True\n",
    "\n",
    "params_dict = {}\n",
    "\n",
    "# Dataset Params - dataset.____ keys are passed into DatasetHandler constructor\n",
    "params_dict[\"dataset.name\"] = [\"smsm\", \"kinetics_100\", \"moma_act\", \"moma_sact\"]\n",
    "params_dict[\"dataset.split\"] = [\"val\"]\n",
    "params_dict[\"dataset.split_type\"] = [\"video\"]\n",
    "\n",
    "# Few-Shot Test Params - test.____ keys are passed into few-shot test call\n",
    "params_dict[\"test.n_way\"] = [None] # None gets converted into the max value for each dataset\n",
    "params_dict[\"test.n_support\"] = [1, 2, 4, 8, 16]\n",
    "params_dict[\"test.n_query\"] = [None]\n",
    "params_dict[\"test.n_episodes\"] = [4]\n",
    "\n",
    "# VLM Params - vlm.____ keys are passed into VLM constructor\n",
    "if ENV == \"VLM_CLIP\":\n",
    "    from CLIP.CLIPVLM import ClipVLM as VLM\n",
    "    params_dict[\"vlm.num_frames\"] = [10]\n",
    "elif ENV == \"VLM_MILES\":\n",
    "    from MILES.wrapper import MILES_SimilarityVLM as VLM\n",
    "elif ENV == \"videoclip\":\n",
    "    from video_clip.video_clip import VideoClipVLM as VLM\n",
    "    params_dict[\"vlm.num_seconds\"] = [4]\n",
    "    params_dict[\"vlm.sample_strat\"] = [\"spread\"]\n",
    "    params_dict[\"vlm.use_cuda\"] = [True]\n",
    "elif ENV == \"VLM_UNIVL\":\n",
    "    from UNIVL.wrapper import UniVL_SimilarityVLM as VLM\n",
    "elif ENV == \"VLM_VTTWINS\":\n",
    "    from VTTWINS.wrapper import VTTWINS_SimilarityVLM as VLM\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Classifier Params - classifier.____ keys are passed into classifier constructor\n",
    "if False:\n",
    "    from classifier import WeightedTextFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 4.0, 10.0, 100.0]\n",
    "    #params_dict[\"classifier.metric\"] = [Similarity.COSINE, Similarity.DOT, Similarity.EUCLID]\n",
    "if False:\n",
    "    from classifier import HardPromptFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 2.0, 4.0, 6.0, 8.0, 10.0, 20.0, 30.0, 40.0, 50.0, 100.0]\n",
    "    params_dict[\"classifier.prompt_text\"] = [\n",
    "        \"\",\n",
    "        \"a photo showing the task of\",\n",
    "        \"an activity of\",\n",
    "        \"the video shows me\"\n",
    "    ]\n",
    "if False:\n",
    "    from classifier import NearestNeighborFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.neighbor_count\"] = [1, 2, 3, 4, 5, 10, 20]\n",
    "    params_dict[\"classifier.neighbor_weights\"] = [\"uniform\", \"distance\"]\n",
    "if False:\n",
    "    from classifier import GaussianFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 4.0, 10.0, 100.0]\n",
    "    params_dict[\"classifier.prior_count\"] = [0, 1, 3, 10, 30, 100]\n",
    "    params_dict[\"classifier.prior_var\"] = [0, 1, 3, 10, 30, 100]\n",
    "if False:\n",
    "    from classifier import SubVideoAverageFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 4.0, 10.0, 100.0]\n",
    "    params_dict[\"classifier.subvideo_segment_duration\"] = [1, 2, 5]\n",
    "    params_dict[\"classifier.subvideo_max_segments\"] = [32]\n",
    "    params_dict[\"classifier.subvideo_discard_proportion\"] = [0, 0.1, 0.25, 0.5]\n",
    "if False:\n",
    "    from classifier import TipAdapterFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.alpha\"] = [858]#[238]#[100, 120, 140] #[0.5, 1.0, 2.0]\n",
    "    params_dict[\"classifier.beta\"] = [26]#[5.07] #[2.5, 5.5, 10.0]\n",
    "    params_dict[\"classifier.finetune_lr\"] = [0]#[5.9e-4]#[1e-4, 3e-4, 1e-3, 3e-3, 1e-2]\n",
    "    params_dict[\"classifier.finetune_epochs\"] = [0]#[10] #[0, 1, 5, 10, 20]\n",
    "if False:\n",
    "    from classifier.smsm_object_oracle import SmsmObjectOracleFewShotClassifier as Classifier\n",
    "if False:\n",
    "    from classifier.coop import CoopFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.lr\"] = [2e-4]#, 2e-3, 1e-2]\n",
    "    params_dict[\"classifier.epochs\"] = [10]\n",
    "    params_dict[\"classifier.warmup_epochs\"] = [1]\n",
    "    params_dict[\"classifier.random_augment\"] = [True]\n",
    "    params_dict[\"classifier.batch_size\"] = [8]\n",
    "if False:\n",
    "    from classifier.cona import CoNaFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.batch_size\"] = [8]\n",
    "    params_dict[\"classifier.random_augment\"] = [False]\n",
    "    params_dict[\"classifier.optimizer\"] = [\"adamw\"]\n",
    "    params_dict[\"classifier.epochs\"] = [20]\n",
    "    params_dict[\"classifier.lr\"] = [4e-4]\n",
    "    params_dict[\"classifier.name_regularization\"] = [1]\n",
    "if False:\n",
    "    from classifier.cona_tip_adapter import CoNaTipAdapterFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.optimizer\"] = [\"adamw\"]\n",
    "    params_dict[\"classifier.batch_size\"] = [8]\n",
    "    params_dict[\"classifier.random_augment\"] = [False]\n",
    "    params_dict[\"classifier.epochs\"] = [20]\n",
    "    params_dict[\"classifier.lr\"] = [4e-4]\n",
    "    params_dict[\"classifier.adapter_lr_multiplier\"] = [1] #[8/256]\n",
    "    params_dict[\"classifier.name_regularization\"] = [10]\n",
    "    params_dict[\"classifier.adapter_regularization\"] = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SAVEFILE = None#\"cona_test.csv\"\n",
    "test_handler = FewShotTestHandler(RUN_SAVEFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960643ce72de4ef7b4bc223708eeb765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######USING ATTENTION STYLE:  frozen-in-time\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9292ba98d0fd4eed9e7cf3a73cd47171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e33ad2895145ce87c00ee45311b32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15daf0d570a497caaa3ac3751655213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a934be67e6b42b5a020aacc18e210e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228c3c94953c439897d890acbe305c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_parameters at 0x7f72d3d2fdd0>\n",
      "[{'params': Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0', requires_grad=True), 'weight_decay': 10.0}, {'params': <generator object Module.parameters at 0x7f72d3d2fd60>, 'lr': 0.0004, 'weight_decay': 10.0}, {'params': Parameter containing:\n",
      "tensor([[[ 0.0218,  0.0097,  0.0024,  ..., -0.0142,  0.0075, -0.0095],\n",
      "         [-0.0199,  0.0294, -0.0143,  ..., -0.0019, -0.0166, -0.0131],\n",
      "         [ 0.0037, -0.0126, -0.0035,  ..., -0.0091, -0.0118, -0.0149],\n",
      "         ...,\n",
      "         [ 0.0120,  0.0040, -0.0207,  ...,  0.0040, -0.0231,  0.0268],\n",
      "         [ 0.0089,  0.0085,  0.0061,  ...,  0.0176, -0.0125, -0.0149],\n",
      "         [-0.0423, -0.0022, -0.0220,  ..., -0.0083, -0.0102, -0.0010]]],\n",
      "       device='cuda:0', requires_grad=True)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniconda3/envs/VLM_MILES/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: Support Acc = 0.030, Val-Tune Acc = 0.025, Loss = 5.696E+00, Name Mag: 6.177E-04, Adapter Mag = 8.670E-04\n",
      "Epoch     1: Support Acc = 0.030, Val-Tune Acc = 0.030, Loss = 4.642E+00, Name Mag: 1.475E-02, Adapter Mag = 2.597E-02\n",
      "Epoch     2: Support Acc = 0.070, Val-Tune Acc = 0.055, Loss = 4.046E+00, Name Mag: 2.279E-02, Adapter Mag = 4.957E-02\n",
      "Epoch     3: Support Acc = 0.310, Val-Tune Acc = 0.059, Loss = 3.384E+00, Name Mag: 2.855E-02, Adapter Mag = 7.142E-02\n",
      "Epoch     4: Support Acc = 0.450, Val-Tune Acc = 0.057, Loss = 2.749E+00, Name Mag: 3.413E-02, Adapter Mag = 9.039E-02\n",
      "Epoch     5: Support Acc = 0.620, Val-Tune Acc = 0.053, Loss = 2.155E+00, Name Mag: 3.904E-02, Adapter Mag = 1.059E-01\n",
      "Epoch     6: Support Acc = 0.730, Val-Tune Acc = 0.055, Loss = 1.583E+00, Name Mag: 4.315E-02, Adapter Mag = 1.178E-01\n",
      "Epoch     7: Support Acc = 0.870, Val-Tune Acc = 0.050, Loss = 1.143E+00, Name Mag: 4.608E-02, Adapter Mag = 1.264E-01\n",
      "Epoch     8: Support Acc = 0.980, Val-Tune Acc = 0.046, Loss = 8.203E-01, Name Mag: 4.796E-02, Adapter Mag = 1.321E-01\n",
      "Epoch     9: Support Acc = 0.990, Val-Tune Acc = 0.051, Loss = 6.999E-01, Name Mag: 4.909E-02, Adapter Mag = 1.357E-01\n",
      "Epoch    10: Support Acc = 1.000, Val-Tune Acc = 0.053, Loss = 4.751E-01, Name Mag: 4.977E-02, Adapter Mag = 1.378E-01\n",
      "Epoch    11: Support Acc = 1.000, Val-Tune Acc = 0.053, Loss = 3.621E-01, Name Mag: 5.003E-02, Adapter Mag = 1.387E-01\n",
      "Epoch    12: Support Acc = 1.000, Val-Tune Acc = 0.050, Loss = 3.003E-01, Name Mag: 5.010E-02, Adapter Mag = 1.389E-01\n",
      "Epoch    13: Support Acc = 1.000, Val-Tune Acc = 0.049, Loss = 2.630E-01, Name Mag: 5.006E-02, Adapter Mag = 1.388E-01\n",
      "Epoch    14: Support Acc = 1.000, Val-Tune Acc = 0.051, Loss = 2.359E-01, Name Mag: 5.002E-02, Adapter Mag = 1.387E-01\n",
      "Epoch    15: Support Acc = 1.000, Val-Tune Acc = 0.052, Loss = 2.215E-01, Name Mag: 4.997E-02, Adapter Mag = 1.386E-01\n",
      "Epoch    16: Support Acc = 1.000, Val-Tune Acc = 0.051, Loss = 2.095E-01, Name Mag: 4.993E-02, Adapter Mag = 1.384E-01\n",
      "Epoch    17: Support Acc = 1.000, Val-Tune Acc = 0.051, Loss = 2.019E-01, Name Mag: 4.991E-02, Adapter Mag = 1.384E-01\n",
      "Epoch    18: Support Acc = 1.000, Val-Tune Acc = 0.052, Loss = 1.983E-01, Name Mag: 4.990E-02, Adapter Mag = 1.383E-01\n",
      "Epoch    19: Support Acc = 1.000, Val-Tune Acc = 0.052, Loss = 1.964E-01, Name Mag: 4.989E-02, Adapter Mag = 1.383E-01\n",
      "<generator object Module.named_parameters at 0x7f72d3d1c270>\n",
      "[{'params': Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0', requires_grad=True), 'weight_decay': 10.0}, {'params': <generator object Module.parameters at 0x7f72d3d1c2e0>, 'lr': 0.0004, 'weight_decay': 10.0}, {'params': Parameter containing:\n",
      "tensor([[[ 0.0025,  0.0175, -0.0118,  ...,  0.0122,  0.0295, -0.0175],\n",
      "         [ 0.0217,  0.0256, -0.0067,  ...,  0.0034, -0.0321, -0.0059],\n",
      "         [ 0.0168, -0.0179,  0.0013,  ..., -0.0093,  0.0148, -0.0300],\n",
      "         ...,\n",
      "         [-0.0261,  0.0156,  0.0043,  ..., -0.0409,  0.0255,  0.0163],\n",
      "         [-0.0492,  0.0011,  0.0296,  ..., -0.0030, -0.0203,  0.0137],\n",
      "         [-0.0340, -0.0405, -0.0038,  ...,  0.0208, -0.0158, -0.0125]]],\n",
      "       device='cuda:0', requires_grad=True)}]\n",
      "Epoch     0: Support Acc = 0.030, Val-Tune Acc = 0.036, Loss = 5.017E+00, Name Mag: 6.636E-04, Adapter Mag = 9.381E-04\n",
      "Epoch     1: Support Acc = 0.060, Val-Tune Acc = 0.047, Loss = 4.551E+00, Name Mag: 1.473E-02, Adapter Mag = 2.735E-02\n",
      "Epoch     2: Support Acc = 0.190, Val-Tune Acc = 0.065, Loss = 3.947E+00, Name Mag: 2.296E-02, Adapter Mag = 5.172E-02\n",
      "Epoch     3: Support Acc = 0.330, Val-Tune Acc = 0.061, Loss = 3.282E+00, Name Mag: 3.019E-02, Adapter Mag = 7.368E-02\n",
      "Epoch     4: Support Acc = 0.390, Val-Tune Acc = 0.052, Loss = 2.549E+00, Name Mag: 3.714E-02, Adapter Mag = 9.230E-02\n",
      "Epoch     5: Support Acc = 0.670, Val-Tune Acc = 0.048, Loss = 1.824E+00, Name Mag: 4.309E-02, Adapter Mag = 1.069E-01\n",
      "Epoch     6: Support Acc = 0.870, Val-Tune Acc = 0.048, Loss = 1.147E+00, Name Mag: 4.746E-02, Adapter Mag = 1.172E-01\n",
      "Epoch     7: Support Acc = 0.960, Val-Tune Acc = 0.041, Loss = 7.807E-01, Name Mag: 5.010E-02, Adapter Mag = 1.240E-01\n",
      "Epoch     8: Support Acc = 0.960, Val-Tune Acc = 0.046, Loss = 5.447E-01, Name Mag: 5.136E-02, Adapter Mag = 1.278E-01\n",
      "Epoch     9: Support Acc = 0.990, Val-Tune Acc = 0.047, Loss = 3.927E-01, Name Mag: 5.205E-02, Adapter Mag = 1.298E-01\n",
      "Epoch    10: Support Acc = 0.990, Val-Tune Acc = 0.059, Loss = 3.348E-01, Name Mag: 5.215E-02, Adapter Mag = 1.306E-01\n",
      "Epoch    11: Support Acc = 1.000, Val-Tune Acc = 0.051, Loss = 2.502E-01, Name Mag: 5.210E-02, Adapter Mag = 1.307E-01\n",
      "Epoch    12: Support Acc = 1.000, Val-Tune Acc = 0.048, Loss = 1.949E-01, Name Mag: 5.191E-02, Adapter Mag = 1.304E-01\n",
      "Epoch    13: Support Acc = 1.000, Val-Tune Acc = 0.048, Loss = 1.731E-01, Name Mag: 5.165E-02, Adapter Mag = 1.299E-01\n",
      "Epoch    14: Support Acc = 1.000, Val-Tune Acc = 0.048, Loss = 1.583E-01, Name Mag: 5.144E-02, Adapter Mag = 1.295E-01\n",
      "Epoch    15: Support Acc = 1.000, Val-Tune Acc = 0.048, Loss = 1.502E-01, Name Mag: 5.129E-02, Adapter Mag = 1.292E-01\n",
      "Epoch    16: Support Acc = 1.000, Val-Tune Acc = 0.049, Loss = 1.442E-01, Name Mag: 5.119E-02, Adapter Mag = 1.290E-01\n",
      "Epoch    17: Support Acc = 1.000, Val-Tune Acc = 0.049, Loss = 1.406E-01, Name Mag: 5.114E-02, Adapter Mag = 1.288E-01\n",
      "Epoch    18: Support Acc = 1.000, Val-Tune Acc = 0.049, Loss = 1.386E-01, Name Mag: 5.111E-02, Adapter Mag = 1.288E-01\n",
      "Epoch    19: Support Acc = 1.000, Val-Tune Acc = 0.049, Loss = 1.375E-01, Name Mag: 5.111E-02, Adapter Mag = 1.288E-01\n",
      "<generator object Module.named_parameters at 0x7f72d3d0f350>\n",
      "[{'params': Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0', requires_grad=True), 'weight_decay': 10.0}, {'params': <generator object Module.parameters at 0x7f72d3d0f2e0>, 'lr': 0.0004, 'weight_decay': 10.0}, {'params': Parameter containing:\n",
      "tensor([[[ 0.0176, -0.0170, -0.0253,  ...,  0.0272, -0.0238, -0.0051],\n",
      "         [-0.0015, -0.0232,  0.0318,  ...,  0.0128, -0.0005,  0.0085],\n",
      "         [-0.0186, -0.0003, -0.0085,  ...,  0.0096,  0.0056,  0.0222],\n",
      "         ...,\n",
      "         [ 0.0416,  0.0095, -0.0077,  ..., -0.0290, -0.0140, -0.0297],\n",
      "         [-0.0124,  0.0033, -0.0102,  ...,  0.0032, -0.0029,  0.0128],\n",
      "         [ 0.0099,  0.0255, -0.0227,  ...,  0.0138, -0.0045, -0.0140]]],\n",
      "       device='cuda:0', requires_grad=True)}]\n",
      "Epoch     0: Support Acc = 0.010, Val-Tune Acc = 0.028, Loss = 5.358E+00, Name Mag: 6.814E-04, Adapter Mag = 9.249E-04\n",
      "Epoch     1: Support Acc = 0.040, Val-Tune Acc = 0.035, Loss = 4.620E+00, Name Mag: 1.561E-02, Adapter Mag = 2.762E-02\n",
      "Epoch     2: Support Acc = 0.140, Val-Tune Acc = 0.046, Loss = 4.057E+00, Name Mag: 2.434E-02, Adapter Mag = 5.270E-02\n",
      "Epoch     3: Support Acc = 0.230, Val-Tune Acc = 0.043, Loss = 3.535E+00, Name Mag: 3.149E-02, Adapter Mag = 7.545E-02\n",
      "Epoch     4: Support Acc = 0.430, Val-Tune Acc = 0.041, Loss = 2.869E+00, Name Mag: 3.834E-02, Adapter Mag = 9.516E-02\n",
      "Epoch     5: Support Acc = 0.700, Val-Tune Acc = 0.045, Loss = 2.231E+00, Name Mag: 4.441E-02, Adapter Mag = 1.116E-01\n",
      "Epoch     6: Support Acc = 0.830, Val-Tune Acc = 0.045, Loss = 1.595E+00, Name Mag: 4.902E-02, Adapter Mag = 1.243E-01\n",
      "Epoch     7: Support Acc = 0.940, Val-Tune Acc = 0.038, Loss = 1.062E+00, Name Mag: 5.209E-02, Adapter Mag = 1.334E-01\n",
      "Epoch     8: Support Acc = 0.950, Val-Tune Acc = 0.038, Loss = 7.612E-01, Name Mag: 5.397E-02, Adapter Mag = 1.390E-01\n",
      "Epoch     9: Support Acc = 1.000, Val-Tune Acc = 0.045, Loss = 5.591E-01, Name Mag: 5.498E-02, Adapter Mag = 1.423E-01\n",
      "Epoch    10: Support Acc = 1.000, Val-Tune Acc = 0.047, Loss = 4.283E-01, Name Mag: 5.546E-02, Adapter Mag = 1.442E-01\n",
      "Epoch    11: Support Acc = 1.000, Val-Tune Acc = 0.042, Loss = 3.246E-01, Name Mag: 5.553E-02, Adapter Mag = 1.449E-01\n",
      "Epoch    12: Support Acc = 1.000, Val-Tune Acc = 0.050, Loss = 2.655E-01, Name Mag: 5.545E-02, Adapter Mag = 1.449E-01\n",
      "Epoch    13: Support Acc = 1.000, Val-Tune Acc = 0.050, Loss = 2.276E-01, Name Mag: 5.528E-02, Adapter Mag = 1.447E-01\n",
      "Epoch    14: Support Acc = 1.000, Val-Tune Acc = 0.047, Loss = 2.072E-01, Name Mag: 5.513E-02, Adapter Mag = 1.445E-01\n",
      "Epoch    15: Support Acc = 1.000, Val-Tune Acc = 0.047, Loss = 1.948E-01, Name Mag: 5.501E-02, Adapter Mag = 1.442E-01\n",
      "Epoch    16: Support Acc = 1.000, Val-Tune Acc = 0.048, Loss = 1.865E-01, Name Mag: 5.493E-02, Adapter Mag = 1.440E-01\n",
      "Epoch    17: Support Acc = 1.000, Val-Tune Acc = 0.047, Loss = 1.804E-01, Name Mag: 5.488E-02, Adapter Mag = 1.439E-01\n",
      "Epoch    18: Support Acc = 1.000, Val-Tune Acc = 0.047, Loss = 1.772E-01, Name Mag: 5.486E-02, Adapter Mag = 1.439E-01\n",
      "Epoch    19: Support Acc = 1.000, Val-Tune Acc = 0.047, Loss = 1.756E-01, Name Mag: 5.486E-02, Adapter Mag = 1.439E-01\n",
      "<generator object Module.named_parameters at 0x7f72d3d2e2e0>\n",
      "[{'params': Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0', requires_grad=True), 'weight_decay': 10.0}, {'params': <generator object Module.parameters at 0x7f72d3d2e350>, 'lr': 0.0004, 'weight_decay': 10.0}, {'params': Parameter containing:\n",
      "tensor([[[ 0.0048,  0.0286, -0.0111,  ...,  0.0073,  0.0209,  0.0275],\n",
      "         [ 0.0240,  0.0321,  0.0018,  ...,  0.0166,  0.0205,  0.0006],\n",
      "         [-0.0188,  0.0207, -0.0122,  ..., -0.0272, -0.0001,  0.0517],\n",
      "         ...,\n",
      "         [-0.0197, -0.0222, -0.0008,  ...,  0.0165,  0.0220, -0.0041],\n",
      "         [-0.0020, -0.0002, -0.0269,  ...,  0.0007,  0.0436,  0.0232],\n",
      "         [-0.0167,  0.0210, -0.0041,  ..., -0.0030, -0.0249,  0.0281]]],\n",
      "       device='cuda:0', requires_grad=True)}]\n",
      "Epoch     0: Support Acc = 0.030, Val-Tune Acc = 0.022, Loss = 5.225E+00, Name Mag: 6.822E-04, Adapter Mag = 9.054E-04\n",
      "Epoch     1: Support Acc = 0.050, Val-Tune Acc = 0.028, Loss = 4.510E+00, Name Mag: 1.576E-02, Adapter Mag = 2.718E-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rob/vlm_benchmark/vlm_param_tests.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     support_dataset\u001b[39m.\u001b[39mfill_cache(vlm)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Run test\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m test_handler\u001b[39m.\u001b[39;49mrun_few_shot_test(classifier, query_dataset, support_dataset, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtest_params, val_tuning_dataset\u001b[39m=\u001b[39;49mval_dataset \u001b[39mif\u001b[39;49;00m USE_VAL_TUNING \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/vlm_benchmark/FewShotTestHandler.py:78\u001b[0m, in \u001b[0;36mFewShotTestHandler.run_few_shot_test\u001b[0;34m(self, classifier, query_dataset, support_dataset, n_way, n_support, n_query, n_episodes, val_tuning_dataset)\u001b[0m\n\u001b[1;32m     75\u001b[0m dataset_iter \u001b[39m=\u001b[39m tqdm(few_shot_dataset, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m category_names, support_vid_paths, query_vid_paths, query_vid_labels, val_tuning_vid_paths, val_tuning_vid_labels \u001b[39min\u001b[39;00m dataset_iter:\n\u001b[0;32m---> 78\u001b[0m     query_predictions \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(category_names, support_vid_paths, query_vid_paths, val_tuning_vid_paths, val_tuning_vid_labels)\n\u001b[1;32m     80\u001b[0m     \u001b[39m# Compute accuracy for this sampled task\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     correct_predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(query_predictions \u001b[39m==\u001b[39m query_vid_labels)\n",
      "File \u001b[0;32m~/vlm_benchmark/classifier/cona_tip_adapter.py:202\u001b[0m, in \u001b[0;36mCoNaTipAdapterFewShotClassifier.predict\u001b[0;34m(self, category_names, support_video_paths, query_video_paths, val_tuning_video_paths, val_tuning_video_labels)\u001b[0m\n\u001b[1;32m    200\u001b[0m     combined_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    201\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m--> 202\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    204\u001b[0m \u001b[39m# Check val-tuning performance\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m val_tuning_dataloader \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/VLM_MILES/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/VLM_MILES/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vlm = None\n",
    "cur_vlm_params = None\n",
    "classifier = None\n",
    "cur_classifier_params = None\n",
    "query_dataset = None\n",
    "support_dataset = None\n",
    "cur_dataset_params = None\n",
    "\n",
    "pbar = tqdm(list(itertools.product(*params_dict.values())))\n",
    "for params in pbar:\n",
    "    # Associate keys to each param\n",
    "    params = dict(zip(params_dict.keys(), params))\n",
    "    \n",
    "    pbar.set_postfix(params)\n",
    "    \n",
    "    # vlm params\n",
    "    vlm_params = {key[4:]: val for key, val in params.items() if key.startswith(\"vlm.\")}\n",
    "    classifier_params = {key[11:]: val for key, val in params.items() if key.startswith(\"classifier.\")}\n",
    "    dataset_params = {key[8:]: val for key, val in params.items() if key.startswith(\"dataset.\")}\n",
    "    test_params = {key[5:]: val for key, val in params.items() if key.startswith(\"test.\")}\n",
    "    \n",
    "    # Update dataset\n",
    "    if query_dataset is None or cur_dataset_params != dataset_params:\n",
    "        query_dataset = DatasetHandler(**dataset_params)\n",
    "        support_dataset_params = dict(dataset_params, split=\"train\")\n",
    "        support_dataset = DatasetHandler(**support_dataset_params)\n",
    "        val_dataset_params = dict(dataset_params, split=\"val\")\n",
    "        val_dataset = DatasetHandler(**val_dataset_params)\n",
    "        \n",
    "        cur_dataset_params = dataset_params\n",
    "        new_dataset = True\n",
    "    else:\n",
    "        new_dataset = False\n",
    "        \n",
    "    # Convert n_way = None into n_way = max-ways\n",
    "    if test_params[\"n_way\"] is None:\n",
    "        test_params[\"n_way\"] = support_dataset.category_count()\n",
    "    \n",
    "    # Update vlm (which forces update of classifier)\n",
    "    if vlm is None or cur_vlm_params != vlm_params:\n",
    "        vlm = VLM(**vlm_params)\n",
    "        \n",
    "        cur_vlm_params = vlm_params\n",
    "        new_vlm = True\n",
    "    else:\n",
    "        new_vlm = False\n",
    "            \n",
    "    if new_vlm or classifier is None or cur_classifier_params != classifier_params:\n",
    "        classifier = Classifier(vlm, **classifier_params)\n",
    "        cur_classifier_params = classifier_params\n",
    "        \n",
    "    # Fill dataset caches\n",
    "    if new_dataset or new_vlm:\n",
    "        query_dataset.fill_cache(vlm)\n",
    "        support_dataset.fill_cache(vlm)\n",
    "    \n",
    "    # Run test\n",
    "    test_handler.run_few_shot_test(classifier, query_dataset, support_dataset, **test_params, val_tuning_dataset=val_dataset if USE_VAL_TUNING else None)\n",
    "    \n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_handler.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    test_handler.results,\n",
    "    x_col=\"n_support\",\n",
    "    y_col=\"accuracy\",\n",
    "    plot_descriptor_cols=[\"query_dataset\", \"classifier_class\"],\n",
    "    line_descriptor_cols=[\"vlm_class\", \"classifier.epochs\", \"classifier.name_regularization\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Best Hyperparameters on Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparam_values = find_hyperparameters(\n",
    "    test_handler.results,\n",
    "    hyperparam_cols=[col for col in test_handler.results if col.startswith(\"classifier.\") or col.startswith(\"vlm.\")]\n",
    ")\n",
    "display(best_hyperparam_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change params_dict to only include dataset and test info, then run tests with best hyperparameter values\n",
    "test_split_params_dict = {\n",
    "    \"dataset.split\": [\"test\"]\n",
    "}\n",
    "for key, val in params_dict.items():\n",
    "    if key.startswith(\"classifier.\") or key.startswith(\"vlm.\") or key == \"dataset.split\":\n",
    "        continue\n",
    "    test_split_params_dict[key] = val\n",
    "    \n",
    "if RUN_SAVEFILE is not None:\n",
    "    test_run_savefile = RUN_SAVEFILE.split(\".\")\n",
    "    test_run_savefile.insert(1, \"test\")\n",
    "    test_run_savefile = \".\".join(test_run_savefile)\n",
    "else:\n",
    "    test_run_savefile = None\n",
    "final_test_handler = FewShotTestHandler(test_run_savefile)\n",
    "    \n",
    "vlm = None\n",
    "cur_vlm_params = None\n",
    "classifier = None\n",
    "cur_classifier_params = None\n",
    "query_dataset = None\n",
    "support_dataset = None\n",
    "cur_dataset_params = None\n",
    "\n",
    "pbar = tqdm(list(itertools.product(*test_split_params_dict.values())))\n",
    "for params in pbar:\n",
    "    # Associate keys to each param\n",
    "    params = dict(zip(test_split_params_dict.keys(), params))\n",
    "    \n",
    "    # Determine dataset and test parameters\n",
    "    dataset_params = {key[8:]: val for key, val in params.items() if key.startswith(\"dataset.\")}\n",
    "    test_params = {key[5:]: val for key, val in params.items() if key.startswith(\"test.\")}\n",
    "    \n",
    "    # Update dataset\n",
    "    if query_dataset is None or cur_dataset_params != dataset_params:\n",
    "        query_dataset = DatasetHandler(**dataset_params)\n",
    "        support_dataset_params = dict(dataset_params, split=\"train\")\n",
    "        support_dataset = DatasetHandler(**support_dataset_params)\n",
    "        \n",
    "        # Construct dummy val dataset to get id for filtering dataframe results with corresponding hyperparameters\n",
    "        # Also for val-tuning if enabled\n",
    "        val_dataset = DatasetHandler(**dict(dataset_params, split=\"val\"))\n",
    "        \n",
    "        cur_dataset_params = dataset_params\n",
    "        new_dataset = True\n",
    "    else:\n",
    "        new_dataset = False\n",
    "        \n",
    "    # Convert n_way = None into n_way = max-ways\n",
    "    if test_params[\"n_way\"] is None:\n",
    "        test_params[\"n_way\"] = support_dataset.category_count()\n",
    "        \n",
    "    # Determine vlm and classifier params from hyperparameter dataframe\n",
    "    matched_hyperparam_values = np.ones(len(best_hyperparam_values)).astype(bool)\n",
    "    matched_hyperparam_values &= (best_hyperparam_values[\"vlm_class\"] == VLM.__name__) & (best_hyperparam_values[\"classifier_class\"] == Classifier.__name__)\n",
    "    matched_hyperparam_values &= (best_hyperparam_values[\"query_dataset\"] == val_dataset.id()) & (best_hyperparam_values[\"support_dataset\"] == support_dataset.id())\n",
    "    for col, val in test_params.items():\n",
    "        if pd.isna(val):\n",
    "            matched_hyperparam_values &= pd.isna(best_hyperparam_values[col])        \n",
    "        else:\n",
    "            matched_hyperparam_values &= (best_hyperparam_values[col] == val)\n",
    "    matched_hyperparam_values = best_hyperparam_values[matched_hyperparam_values].reset_index(drop=True)\n",
    "    \n",
    "    vlm_params = {}\n",
    "    classifier_params = {}\n",
    "    for col in matched_hyperparam_values.columns:\n",
    "        if col.startswith(\"vlm.\"):\n",
    "            val = matched_hyperparam_values.loc[0, col]\n",
    "            if not pd.isna(val):\n",
    "                vlm_params[col[4:]] = val\n",
    "        \n",
    "        if col.startswith(\"classifier.\"):\n",
    "            val = matched_hyperparam_values.loc[0, col]\n",
    "            if not pd.isna(val):\n",
    "                if col != \"classifier.metric\":\n",
    "                    classifier_params[col[11:]] = val\n",
    "                else:\n",
    "                    classifier_params[col[11:]] = Similarity[val]\n",
    "        \n",
    "    for key, val in vlm_params.items():\n",
    "        params[f\"vlm.{key}\"] = val\n",
    "    for key, val in classifier_params.items():\n",
    "        params[f\"classifier.{key}\"] = val\n",
    "    pbar.set_postfix(params)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update vlm (which forces update of classifier)\n",
    "    if vlm is None or cur_vlm_params != vlm_params:\n",
    "        vlm = VLM(**vlm_params)\n",
    "        \n",
    "        cur_vlm_params = vlm_params\n",
    "        new_vlm = True\n",
    "    else:\n",
    "        new_vlm = False\n",
    "            \n",
    "    if new_vlm or classifier is None or cur_classifier_params != classifier_params:\n",
    "        classifier = Classifier(vlm, **classifier_params)\n",
    "        cur_classifier_params = classifier_params\n",
    "        \n",
    "    # Fill dataset caches\n",
    "    if new_dataset or new_vlm:\n",
    "        query_dataset.fill_cache(vlm)\n",
    "        support_dataset.fill_cache(vlm)\n",
    "    \n",
    "    # Run test\n",
    "    final_test_handler.run_few_shot_test(classifier, query_dataset, support_dataset, **test_params, val_tuning_dataset=val_dataset if USE_VAL_TUNING else None)\n",
    "    \n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_test_handler.results.sort_values([\"query_dataset\", \"n_support\", \"vlm_class\"])[[\"query_dataset\", \"n_support\", \"vlm_class\", \"accuracy\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    final_test_handler.results,\n",
    "    x_col=\"n_support\",\n",
    "    y_col=\"accuracy\",\n",
    "    plot_descriptor_cols=[\"query_dataset\", \"classifier_class\"],\n",
    "    line_descriptor_cols=[\"vlm_class\", \"classifier.epochs\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('VLM_MILES')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa12131d24cc94205087ad381a7dfdae34f6e827b859c030f23450f0750c1ff3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
